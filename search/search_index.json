{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Swiss Territorial Data Lab - STDL \u00b6 The STDL aims to promote collective innovation around the Swiss territory and its digital copy. It mainly explores the possibilities provided by data science to improve official land registering. A multidisciplinary team composed of cantonal, federal and academic partners is reinforced by engineers specialized in geographical data science to tackle the challenges around the management of territorial data-sets. The developed STDL platform codes and documentation are published under open licenses to allow partners and Swiss territory management actors to leverage the developed technologies. Exploratory Projects \u00b6 Exploratory projects in the field of the Swiss territorial data are conducted at the demand of institutions or actors of the Swiss territory. The exploratory projects are conducted with the supervision of the principal in order to closely analyze the answers to the specifications along the project. The goal of exploratory project aims to provide proof-of-concept and expertise in the application of technologies to Swiss territorial data. COMPLETION OF THE FEDERAL REGISTER OF BUILDINGS AND DWELLINGS Nils Hamel (UNIGE) - Huriel Reichel (swisstopo) Proposed by the Federal Statistical Office - TASK-REGBL The Swiss Federal Statistical Office is in charge of the national Register of of Buildings and Dwellings (RBD) which keep tracks of every existing building in Switzerland. Currently, the register is being completed with building in addition to regular dwellings to offer a reliable and official source of information. The completion of the register introduced issue dues to missing information and their difficulty to be collected. The construction years of the building is one missing information for large amount of register entries. The Statistical Office mandated the STDL to investigate on the possibility to use the Swiss National Maps to extract this missing information using an automated process. A research was conducted in this direction with the development of a proof-of-concept and a reliable methodology to assess the obtained results. DIFFERENCE MODELS APPLIED ON LAND REGISTER Nils Hamel (UNIGE) - Huriel Reichel (swisstopo) Project scheduled in the STDL research roadmap - TASK-DTRK Being able to track modifications in the evolution of geographical datasets is one important aspect in territory management, as a large amount of information can be extracted out of differences models. Differences detection can also be a tool used to assess the evolution of a geographical model through time. In this research project, we apply differences detection on INTERLIS models of the official Swiss land registers in order to emphasize and follow its evolution and to demonstrate that change in reference frames can be detected and assessed. Research Developments \u00b6 Research developments are conducted aside of the research projects to provide a framework of tools and expertise around the Swiss territorial data and related technologies. The research developments are conducted according to the research plan established by the data scientists and validated by the steering committee. AUTOMATIC DETECTION OF CHANGES IN THE ENVIRONMENT Nils Hamel (UNIGE) Project scheduled in the STDL research roadmap - TASK-DIFF Developed at EPFL with the collaboration of Cadastre Suisse to handle large scale geographical models of different nature, the STDL 4D platform offers a robust and efficient indexation methodology allowing to manage storage and access to large-scale models. In addition to spatial indexation, the platform also includes time as part of the indexation, allowing any area to be described by models in both spatial and temporal dimensions. In this development project, the notion of model temporal derivative is explored and proof-of-concepts are implemented in the platform. The goal is to demonstrate that, in addition to their formal content, models coming with different temporal versions can be derived along the time dimension to compute difference models. Such proof-of-concept is developed for both point cloud and vectorial models, demonstrating that the indexation formalism of the platform is able to ease considerably the computation of difference models. This research project demonstrates that the time dimension can be fully exploited in order to access the data it holds. Steering Committee \u00b6 The steering committee of the Swiss Territorial Data Lab is composed of Swiss Institutions bringing their expertise and competences to define the research strategy and to guide the conducted projects and developments. Partners of the STDL and members of the steering committee Proposing a Research Project \u00b6 To propose a research project to the STDL , simply contact us using the following email address and by providing a first small description of the project, the possible data to be considered and initial expected specifications. info@stdl.ch The STDL will contact you soon after a first overview of the proposed research project.","title":"Swiss Territorial Data Lab - STDL"},{"location":"#swiss-territorial-data-lab-stdl","text":"The STDL aims to promote collective innovation around the Swiss territory and its digital copy. It mainly explores the possibilities provided by data science to improve official land registering. A multidisciplinary team composed of cantonal, federal and academic partners is reinforced by engineers specialized in geographical data science to tackle the challenges around the management of territorial data-sets. The developed STDL platform codes and documentation are published under open licenses to allow partners and Swiss territory management actors to leverage the developed technologies.","title":"Swiss Territorial Data Lab - STDL"},{"location":"#exploratory-projects","text":"Exploratory projects in the field of the Swiss territorial data are conducted at the demand of institutions or actors of the Swiss territory. The exploratory projects are conducted with the supervision of the principal in order to closely analyze the answers to the specifications along the project. The goal of exploratory project aims to provide proof-of-concept and expertise in the application of technologies to Swiss territorial data. COMPLETION OF THE FEDERAL REGISTER OF BUILDINGS AND DWELLINGS Nils Hamel (UNIGE) - Huriel Reichel (swisstopo) Proposed by the Federal Statistical Office - TASK-REGBL The Swiss Federal Statistical Office is in charge of the national Register of of Buildings and Dwellings (RBD) which keep tracks of every existing building in Switzerland. Currently, the register is being completed with building in addition to regular dwellings to offer a reliable and official source of information. The completion of the register introduced issue dues to missing information and their difficulty to be collected. The construction years of the building is one missing information for large amount of register entries. The Statistical Office mandated the STDL to investigate on the possibility to use the Swiss National Maps to extract this missing information using an automated process. A research was conducted in this direction with the development of a proof-of-concept and a reliable methodology to assess the obtained results. DIFFERENCE MODELS APPLIED ON LAND REGISTER Nils Hamel (UNIGE) - Huriel Reichel (swisstopo) Project scheduled in the STDL research roadmap - TASK-DTRK Being able to track modifications in the evolution of geographical datasets is one important aspect in territory management, as a large amount of information can be extracted out of differences models. Differences detection can also be a tool used to assess the evolution of a geographical model through time. In this research project, we apply differences detection on INTERLIS models of the official Swiss land registers in order to emphasize and follow its evolution and to demonstrate that change in reference frames can be detected and assessed.","title":"Exploratory Projects"},{"location":"#research-developments","text":"Research developments are conducted aside of the research projects to provide a framework of tools and expertise around the Swiss territorial data and related technologies. The research developments are conducted according to the research plan established by the data scientists and validated by the steering committee. AUTOMATIC DETECTION OF CHANGES IN THE ENVIRONMENT Nils Hamel (UNIGE) Project scheduled in the STDL research roadmap - TASK-DIFF Developed at EPFL with the collaboration of Cadastre Suisse to handle large scale geographical models of different nature, the STDL 4D platform offers a robust and efficient indexation methodology allowing to manage storage and access to large-scale models. In addition to spatial indexation, the platform also includes time as part of the indexation, allowing any area to be described by models in both spatial and temporal dimensions. In this development project, the notion of model temporal derivative is explored and proof-of-concepts are implemented in the platform. The goal is to demonstrate that, in addition to their formal content, models coming with different temporal versions can be derived along the time dimension to compute difference models. Such proof-of-concept is developed for both point cloud and vectorial models, demonstrating that the indexation formalism of the platform is able to ease considerably the computation of difference models. This research project demonstrates that the time dimension can be fully exploited in order to access the data it holds.","title":"Research Developments"},{"location":"#steering-committee","text":"The steering committee of the Swiss Territorial Data Lab is composed of Swiss Institutions bringing their expertise and competences to define the research strategy and to guide the conducted projects and developments. Partners of the STDL and members of the steering committee","title":"Steering Committee"},{"location":"#proposing-a-research-project","text":"To propose a research project to the STDL , simply contact us using the following email address and by providing a first small description of the project, the possible data to be considered and initial expected specifications. info@stdl.ch The STDL will contact you soon after a first overview of the proposed research project.","title":"Proposing a Research Project"},{"location":"TASK-4RAS/","text":"TASK-4RAS - HR, NH \u00b6 Schedule : September 2020 to February 2021 (initially planned from August 2021 February 2022) This document describe the state of an ongoing task (DIFF) and is subject to daily revision and evolution Context \u00b6 The 4D platform developed at EPFL with the collaboration of Cadastre Suisse is able to ingest both large scale point-based and vector-based models. During the previous development, the possibility to have this different type of data in a single framework lead to interesting results, showing the interest to have the possibility to put this different type of data into perspectives. Illustrations of mixed models in the 4D platform : INTERLIS, Mesh and LIDAR - Data : SITN Taking into account point-based and vector-based model allows to almost cover all type of data that are traditionally considered for land registering. The only type of data that is currently missing is the two-dimensional rasters. Indeed, due to their nature, image are more complicated to put in perspective of other three-dimensional data. The goal of this task is then to address the management of the raster by the platform in order to be able to ingest, store and broadcast any type of data with the 4D platform. Specifications \u00b6 In order to address this task, a step-by-step approach is defined. In the first place, a set of data has to be gathered from the STDL partners : Gathering a dataset of geo-referenced ortho-photography of a chosen place of reasonable size The dataset has to provide ortho-photography for at least two different times The format of the dataset has to be analyzed in order to be able to extract the image pixels with their position (CH1903+) As the platform indexation formalism is not straightforward, the images are treated as point-based model, each pixel being one colored point of the model. This will allow to provide a way of starting to analyze and understand the indexation formalism while having first results on image integration : Transform images into simple point-based models (each pixel being one point) Injection of the point-based model in an experimental instance of the platform Understanding the indexation formalism for point-based models and, subsequently, its adaptation for the vector-based models As the indexation formalism is understood for point-based models, the following adaptation will be performed : removing the third dimension from the point-based indexation specifically for the image (flat indexation) At this point, a first reporting is required : Is there an advantage to add raster to such a platform in perspective of the other types of model (points, vectors, meshes) ? How the adaptation of the point-based indexation performs for images ? How taking advantage of color accumulation enrich the image integration ? What is the cost of rendering the image with the adaptation of the point-based indexation ? Based on the formulated answer, the following strategical choice has to be discussed : Would it be more efficient to integrate image keeping them as raster (deviation from the current indexation) ? Depending on the answer, a new set of specification will be decided (if this direction is favored). Depending on the remaining time and on the obtained results, the question of the time management in the platform will be addressed. Currently, the time is treated linearly in the platform and a multi-scale approach, as for the spatial dimensions, could be interesting. The specifications will be decided as the previous points will be fulfilled. Resources \u00b6 List of the resources initially linked to the task : liberatosthene - Platform and indexation back-end eratosthene-suite - Platform front-end Other resources will be provided according to requirements.","title":"**TASK-4RAS** - HR, NH"},{"location":"TASK-4RAS/#task-4ras-hr-nh","text":"Schedule : September 2020 to February 2021 (initially planned from August 2021 February 2022) This document describe the state of an ongoing task (DIFF) and is subject to daily revision and evolution","title":"TASK-4RAS - HR, NH"},{"location":"TASK-4RAS/#context","text":"The 4D platform developed at EPFL with the collaboration of Cadastre Suisse is able to ingest both large scale point-based and vector-based models. During the previous development, the possibility to have this different type of data in a single framework lead to interesting results, showing the interest to have the possibility to put this different type of data into perspectives. Illustrations of mixed models in the 4D platform : INTERLIS, Mesh and LIDAR - Data : SITN Taking into account point-based and vector-based model allows to almost cover all type of data that are traditionally considered for land registering. The only type of data that is currently missing is the two-dimensional rasters. Indeed, due to their nature, image are more complicated to put in perspective of other three-dimensional data. The goal of this task is then to address the management of the raster by the platform in order to be able to ingest, store and broadcast any type of data with the 4D platform.","title":"Context"},{"location":"TASK-4RAS/#specifications","text":"In order to address this task, a step-by-step approach is defined. In the first place, a set of data has to be gathered from the STDL partners : Gathering a dataset of geo-referenced ortho-photography of a chosen place of reasonable size The dataset has to provide ortho-photography for at least two different times The format of the dataset has to be analyzed in order to be able to extract the image pixels with their position (CH1903+) As the platform indexation formalism is not straightforward, the images are treated as point-based model, each pixel being one colored point of the model. This will allow to provide a way of starting to analyze and understand the indexation formalism while having first results on image integration : Transform images into simple point-based models (each pixel being one point) Injection of the point-based model in an experimental instance of the platform Understanding the indexation formalism for point-based models and, subsequently, its adaptation for the vector-based models As the indexation formalism is understood for point-based models, the following adaptation will be performed : removing the third dimension from the point-based indexation specifically for the image (flat indexation) At this point, a first reporting is required : Is there an advantage to add raster to such a platform in perspective of the other types of model (points, vectors, meshes) ? How the adaptation of the point-based indexation performs for images ? How taking advantage of color accumulation enrich the image integration ? What is the cost of rendering the image with the adaptation of the point-based indexation ? Based on the formulated answer, the following strategical choice has to be discussed : Would it be more efficient to integrate image keeping them as raster (deviation from the current indexation) ? Depending on the answer, a new set of specification will be decided (if this direction is favored). Depending on the remaining time and on the obtained results, the question of the time management in the platform will be addressed. Currently, the time is treated linearly in the platform and a multi-scale approach, as for the spatial dimensions, could be interesting. The specifications will be decided as the previous points will be fulfilled.","title":"Specifications"},{"location":"TASK-4RAS/#resources","text":"List of the resources initially linked to the task : liberatosthene - Platform and indexation back-end eratosthene-suite - Platform front-end Other resources will be provided according to requirements.","title":"Resources"},{"location":"TASK-DIFF/","text":"AUTOMATIC DETECTION OF CHANGES IN THE ENVIRONMENT \u00b6 Nils Hamel (UNIGE) Project scheduled in the STDL research roadmap - TASK-DIFF September 2020 to November 2020 - Published on December 7, 2020 Abstract : Developed at EPFL with the collaboration of Cadastre Suisse to handle large scale geographical models of different nature, the STDL 4D platform offers a robust and efficient indexation methodology allowing to manage storage and access to large-scale models. In addition to spatial indexation, the platform also includes time as part of the indexation, allowing any area to be described by models in both spatial and temporal dimensions. In this development project, the notion of model temporal derivative is explored and proof-of-concepts are implemented in the platform. The goal is to demonstrate that, in addition to their formal content, models coming with different temporal versions can be derived along the time dimension to compute difference models. Such proof-of-concept is developed for both point cloud and vectorial models, demonstrating that the indexation formalism of the platform is able to ease considerably the computation of difference models. This research project demonstrates that the time dimension can be fully exploited in order to access the data it holds. Task Context : Difference Detection \u00b6 As the implemented indexation formalism is based on equivalences classes defined on space and time, a natural discretization along all the four dimensions is obtained. In the field of difference detection, it allowed implementing simple logical operators on the four-dimensional space. The OR , AND and XOR operators were then implemented allowing the platform to compute, in real time , convolutions to compare models with each others across the time. The implementation of these operators was simple due to the natural spatio-temporal discretization obtained from the indexation formalism. Nevertheless, two major drawbacks appeared : the first one is that such operators only works for point-based models. Having the opportunity to compute and render differences and similarities between any type of data is not possible with such formal operators. The second drawback comes from the nature of the point-based capturing devices. Indeed, taking the example of a building, even without any change to its structure, two digitization campaigns can lead to disparities only due to measures sampling. The XOR operator is the natural choice to detect and render differences, but this operator is very sensitive to sampling disparities. Computing the XOR convolution between two point-based models leads the rendering to be dominated by sampling variations rather than the desired structural differences. This drawback was partially solved by considering the AND operator. Indeed, the AND operator allows to only shows constant structural elements from two different positions in time and is insensitive to sampling disparities. As shown on the following images, the AND operator shows differences as black spots (missing parts) : AND convolution between two LIDAR models : Geneva 2005 and 2009 - Data : SITG As one can see, AND convolutions allow detecting, through the black spots, large area of structural changes between the two times and also, with more care, allow guessing smaller differences. Nevertheless, reading and interpreting such representation remains complex for users. The goal of this task is then to tackle these two drawbacks, allowing the platform to detect changes not only for point-based models but also for vector-based models and to implement a variation of the XOR operator for point-based models allowing to efficiently highlight the structural evolution. The task consists then in the implementation, testing and validation of a difference detection algorithm suitable for any type of model and to conduct a formal analysis on the best rendering techniques. Methodology \u00b6 A step by step methodology is defined to address the problem of difference detection in the platform. In a first phase, the algorithm will be developed and validated on vector-based models as follows : Obtaining a large scale vector-based model on which synthetic variation are introduced Development of the algorithm using the synthetic variations model Testing and validation of the algorithm (using the known synthetic variations) First conclusion In a second phase, true land register data will be used to formally detect real evolutions of the territory : Obtaining true land register vector-based models (INTERLIS) at different times Analysis of the difference detection algorithm on true land register vector-based models Second conclusion In a third phase, the algorithm will be validated and adapted to work on point-based models : Obtaining true land register point-based models (LAS) at different position in time Verifying the performances of the vector-based detection algorithm on point-based data Adaptation of the algorithm for point-based models Analysis of the difference detection algorithm on true land register point-based models Comparison of the detected differences on point-based models and on their corresponding land register vector-based models (INTERLIS) Third conclusion In addition, the development of difference detection algorithm has to be conducted keeping in mind the possible future evolutions of the platform such as addition of layers (separation of data), implementation of a multi-scale approach of the time dimension and addition of raster data in the platform. First Phase : Synthetic Variations \u00b6 In order to implements the vector-based difference detection algorithm, sets of data are considered as base on which synthetic differences are applied to simulate the evolution of the territory. This approach allows focusing on well controlled data to formally benchmark the results of the implemented algorithm. Experiments are conducted using these data to formally evaluate the performance of the developed algorithm. Selected Resources and Models \u00b6 Vector Models : Line-based In this first phase, line-based data are gathered from openstreetmap in order to create simple models used during the implementation and validation of the detection algorithm. A first set of vector-based models are considered made only of lines. Three sets are created each with a different scale, from city to the whole Switzerland. The line-based sets of data are extracted from openstreetmap shapefiles and the elevations are restored using the SRTM geotiff data. The EGM96-5 geoid model is then used to convert the elevation from MSL to ellipsoid heights. The following images give an illustration of these sets of data : Line-based data-sets : Switzerland - Data : OSM The following table gives a summary of the models sizes and primitives count : Model Size (UV3) Primitive Count Frauenfeld 5.0 Mio 93.3 K-Lines Neuch\u00e2tel 33.1 Mio 620.2 K-Lines Switzerland 1.3 Gio 25.0 M-Lines In order to simulate evolution of the territory in time, synthetic variations are added to these models. A script is developed and used to insert controlled variations on selected primitives. The script works by randomly selecting a user-defined amount of primitives of a model and by adding a variation on one of its vertex position using a user-specified amplitude. The variation is applied on the three dimensions of space. Vector Models : Triangle-based A second set of triangle-based models is also considered for implementing and validating the difference detection algorithm. The selected model is a mesh model of the Swiss buildings provided by swisstopo . It comes aligned in the CH1903+ frame with elevations. It is simply converted into the WGS84 frame using again the EGM96-5 geoid model : Triangle-based data-sets : Switzerland - Data : swisstopo The following table gives a summary of the models sizes and primitives count : Model Size (UV3) Primitive Count Frauenfeld 116.9 Mio 1.4 M-Triangles Neuch\u00e2tel 842.2 Mio 10.5 M-Triangles Switzerland 30.5 Gio 390.6 M-Triangles These models are very interesting for difference detection as the ratio between primitive size and model amplitude is very low. It means that all the primitives are small according to the model coverage, especially for the Switzerland one. The developed script for line-based models is also used here to add synthetic variations to the models primitives in order to simulate an evolution of the territory. Models : Statistical Analysis Before using the models in the following developments, a statistical analysis is performed on the two Switzerland models, line and triangle-based. Each primitive of these two models are considered and their edges size are computed to deduce their distribution : Statistical analysis : Models primitive edge size distribution, in meters, for the Switzerland models : line-based (left) and triangle-based (right) One can see that the line-based model comes with a much more broad distribution of the primitives size . Most of the model is made from lines between zero and twenty meters. In the case of the triangle-based models, the primitives are much smaller. As most of them are less than ten meters, a significant fraction of primitives is below one meter. Implementation of the Algorithm \u00b6 In order to compare two models at two different positions in time to detect differences, the solution is of course to search for each primitive of the primary time if it has a corresponding one in the secondary time. In such case, the primitives can be concluded as static in time and only the primitives that have no correspondence will be highlighted as differences. A first approach was initially tested : a vertex-based comparison. As every primitive (points, lines and triangles) is supported by vertexes, it can be seen as a common denominator on which comparison can take place. Unfortunately, it is not a relevant approach as it leads to an asymmetric detection algorithm. To illustrate the issue, the following image shows the situation of a group of line-based primitives at two different times with an evolution on one of the primitive vertex : Asymmetric approach : The variation is detected only when comparing backward in time When the comparison occurs between the second time and the first one, the modified vertex correspondence is not found, and the vertex can be highlighted as a difference. The asymmetry appears as the first time is compared to the second one. In this case, despite the primitive vertex changed, the vertex-based approach is able to find another vertex, part of another primitive, and interprets it as a vertex identity, leading the modified primitive to be considered as static. In order to obtain a fully symmetric algorithm, that does not depend on the way models are compared in times, a primitive-attached approach is considered. The implemented algorithm then treats the correspondence problem from the whole primitive point of view, by checking that the whole primitive can be found in the other model to which it is compared to. This allows to highlight any primitive showing a modification, regardless of the way models are compared and the nature of the modification. In addition to highlighting the primitives that changed through time, the implemented algorithm also renders the primitives that have not changed. The primitives are then shown by modulating their color to emphasize the modifications by keeping their original color for the modified one, while the static primitives are shown in dark gray. This allows to not only show the modifications but also to keep the context of the modifications, helping the user to fully understand the nature of the territory evolution. In addition to color modulation, a variation of difference rendering is analyzed. In addition to color modulation, a visual and artificial marker is added to ease their search. The visual marker is a simple line emanating from the primitive and goes straight up with a size of 512 meters. Such markers are introduced to ease the detection of small primitives that can be difficult to spot according to large point of views. Additional developments were required for triangle-based models : indeed, such models need to be subjected to a light source during rendering for the user to understand the model (face shading). The previously implemented lighting model is then modified to take into account color modulation in order to correctly render the triangle that are highlighted. Moreover, the lighting model was modified to light both face of the triangles in order to light them regardless of the point of view. In addition, as mesh models are made of triangles, primitives can hide themselves. It can then be difficult for the user to spot the highlighted primitives as they can be hidden by others. An option was added to the rendering client allowing the user to ask the rendering of triangles as line-loops or points in order to make them transparent. Finally, an option allowing the user to enable or disable the render face culling was added for him to be able to see the primitive from backward. Results and Experiments \u00b6 With the implemented algorithm, a series of experiments are conducted in order to validate its results and to analyze the efficiency of the difference detection and rendering from a user point of view. In addition, experiments are also conducted to quantify the efficiency of the difference detection for automated processes. Difference Detection : Overview Considering the selected data-sets, each original model is injected at a given time and synthetic variations are added to a copy of it to create a second model injected at another time. The synthetic variations are randomly added to a small amount of primitives of the original model and are of the order of one meter. On the following examples, the detection is operated considering the original model as primary and the modified one as secondary. The following images show examples of how the detection algorithm allows to highlight the detected differences while keeping the rest of the model using a darker color in case of line-based models : Example of difference detection on line-based Frauenfeld (left) and Neuch\u00e2tel (right) models - Data : OSM One can see how the modified primitives are highlighted while keeping the context of the modifications. The highlighted primitive is the one belonging to the primary time. Comparing the models in the other way around would lead the secondary model primitives to be highlighted. Considering the Frauenfeld example, the following images show the situation in the primary time (original model) and the secondary time (model with synthetic variations) : Primary model (left) and secondary one (right) showing the formal situations - The modified primitive is circled in read - Data : OSM As a result, the user can choose between the differences highlighting with the choice of model as primary and can also switch back and worth between the models themselves though the platform interface. Of course, the readability of the difference detection models depends on the size of the modified primitive and the scale at which the model is looked at by the user. If the user adopts a large scale point of view, the differences, even highlighted, can become difficult to spot. This issue can be worsened as triangle-based models are considered. In addition to primitive size, triangles also bring occlusions. The visual markers added to the highlighted primitives can considerably improve ease of differences search by the user. The following images give an example of difference detection without and with the visual markers added by the algorithm : Example of highlighted primitives without (left) and with (right) visual markers - Data : OSM Considering the triangle-based models, difference detection is made more complicated by at least three aspects : the first one is that 3D vector models are more complex than 2D ones in the way primitives (triangles) are more densely packed in the same regions of space in order to correctly model the buildings. The second one is that triangles are solid primitives that bring occlusions in the rendering, hiding other primitives. The last aspect is that such a model can contain very small primitives in order to model the details of the buildings. In such a case, the primitives can be difficult to see, even when highlighted. The following images show an example of highlighted triangles on the Frauenfeld model : Example of highlighted primitive on the Frauenfeld building model - Data : swisstopo On the right image above, the highlighted triangle is underneath the roof of the house, forcing the user to adopt an unconventional point of view (from above the house) to see it. In addition, some primitives can be defined fully inside a volume close by triangles, making them impossible to see without going inside the volume or playing with triangle rendering mode. In such a context, the usage of the visual markers become very important for such models coming with large amount of occlusion and small primitives : Example of highlighted primitives without (left) and with (right) visual markers - Data : swisstopo In case of triangle-based models, the usage of markers appears to be mandatory in order for the user to be able to locate the position of the detected differences in a reasonable amount of time. Difference Detection : User-Based Experiments In any case, for both line and triangle-based models, the difference detection algorithm is only able to highlight visible primitives. Depending on the point of view of the user, part of the primitives are not provided by the platform because of their small size. Indeed, the whole point of the platform is to allow the user to browse through arbitrary large models, which implies to provided only the relevant primitives according to its point of view. As a result, the detection algorithm will not be able to highlight the variations as the involved primitives are not considered as a query answer by the platform. The user has then to reduce is point of view in order to zoom on the small primitives to make them appear, and so, allowing the algorithm to highlight them. In order to show this limitation, an experiment is performed. For each model, a copy is made on which eight synthetic differences are randomly introduced. The variations are of the order of one meter. The models and their modulated copy are injected in the platform. The rule is the following : the user uses the detection algorithm on each model and its modulated copy and has five minutes to detect the eight differences. Each time a difference is seen by the user, the detection time is kept. The user is allowed to use the platform in the way he wants. In each case, the experiment is repeated five times to get a mean detection rate. As one could ask, these measures are made by the user and are difficult to understand without a reference. In order to provide such reference, the following additional experiment is conducted : each model and its modulated copy are submitted to a naive automated detection process. This process parses each primitive of the original model to search in its modulated copy if the primitive appear. If the primitive is not found, the process trigger a difference detection. This process is called naive as it simply implements two nested loops, which is the simplest searching algorithm implementation. The process is written in C with full code optimization and executed by a single thread. Starting with the line-based models, the following figures shows the difference detection rates according to time. For each of the three models, the left plots show the rate without visual markers, the middle ones with visual markers and the right ones the naive process detection rate : Frauenfeld : The black curve shows the mean detection rate while the blue (left, middle) and red (right) area gives the worst and best rates Left : without visual markers - Middle : with visual markers - Right : automated process Canton of Neuch\u00e2tel : The black curve shows the mean detection rate while the blue (left, middle) and red (right) area gives the worst and best rates Left : without visual markers - Middle : with visual markers - Right : automated process Switzerland : The black curve shows the mean detection rate while the blue (left, middle) and red (right) area gives the worst and best rates Left : without visual markers - Middle : with visual markers - Right : automated process As expected, the larger the model is, the more difficult it is for the user to find the highlighted differences, with or without visual markers. Considering a city, the differences, even of the order of one meter, are easy to spot quickly. As the model gets larger, the more time it takes for the user to find the differences. On a model covering a whole canton (Neuch\u00e2tel), one can see that most of the differences are detected in a reasonable amount of time despite their small size according to the overall model. On the Swiss model, things get more complicated, as simply looking at each part of the country is already complicated in only five minutes, leading the detection rate to be lower, even using the visual markers. These results are consistent with the statistical analysis made on the line-based Switzerland model. Detection on a city or even a whole canton lead the user to adopt a point of view sufficiently close to make most of the primitives appearing. For the Switzerland model, the user is forced to adopt a larger point of view, leading to a significant proportion of primitives to stay hidden. These results also show that adding visual markers to the highlighted primitives increases the user detection rate, meaning that the markers lead to a more suitable rendering from the user experience point of view. Considering the user results and the naive detection process, one can see that the user obtains at least similar results but most of the time outperforms the automated process. This allows to demonstrate how the implementation and data broadcasting strategy of the platform is able to provide an efficient way to access models and composite models, here in the context of difference detection. The following figures show the experiments results for the triangle-based models, which were not performed on the whole Switzerland model due to limited rendering capabilities : Frauenfeld : The black curve shows the mean detection rate while the blue (left, middle) and red (right) area gives the worst and best rates Left : without visual markers - Middle : with visual markers - Right : automated process Canton of Neuch\u00e2tel : The black curve shows the mean detection rate while the blue (left, middle) and red (right) area gives the worst and best rates Left : without visual markers - Middle : with visual markers - Right : automated process Similar conclusions apply for the triangle-based models : the larger the model is, the more difficult the difference detection is. These results also confirm that adding visual markers in addition to primitives highlighting significantly helps the user, particularly in case of triangle-based models. The obtained results on triangle-based models are lower than for line-based models. A first explanation is the greater amount of primitive that lead the user to spend more time at each successive point of view. The occlusion problem also seems to play a role, but to a lesser extent as the visual markers seems to largely solve it. The differences between detection on line and triangle-based models have to be searched in the statistical analysis of the triangle-based models. Indeed, for these models, a large proportion of the primitives are very small (less than a meter), leading them to be rendered only as the user adopts a close point of view, making the detection much more complicated in such a small amount of time. The triangle-based models being larger than the line-based one, the results of the naive process are very poor. As for the line-based models experiments, the user outperforms this automated process, in a much more significant way. Difference Detection : Process-Based Experiments In the previous experiments, the user ability to find the differences on the data-sets, using synthetic variations, was benchmark in perspective of the results provided by a naive automated process. The user performs quite well using the platform, but start to struggle as the data-sets get bigger according to the sizes of their primitives. In this second set of experiments, the platform is used through an automated process instead of a user. The process has the same task as the user, that is, finding the eight synthetic differences introduced in the models copy. The process starts with a list of index (the discretization cells of the platform) in order to query the corresponding data to the platform before to search for differences in each cell. The process implements, then, a systematic difference detection covering the whole model. In order for the process to work, it requires an input index list. To create it, the primitive injection condition of the platform is used to determine the maximal depth of these index. The following formula gives the poly-vertex (lines and triangles) primitives injection condition according to the platform scale. In other words, the formula gives the shallowest scale at which the primitive is considered through queries according to its size : where s gives the shallowest scale, R being the WGS84 major semi-axis and e is the largest distance, in meters, between the primitive first vertex and its other ones. For example, choosing s = 26 allows the index to reach any primitive that is greater than ~30 cm over the whole model covered by the index. The scale 26 is then chosen as the deepest search scale in the following experiments. This value can be adapted according to the primitives size and to the nature of the detection process. The larger it is, the more data are broadcast by the platform increasing the processing time. In order to compare the user-based experiments, the naive automated approach and this process-based exhaustive search, the same protocol is considered. The process addresses queries to the platform, based on the index list, and save the detection time of each difference. The detection rate is plot in the same way as for the previous experiments. Again, eight synthetic differences are randomly introduced and the experiment is repeated five times for the line-based model and only two times for the triangle-based model. As the scale 26 is chosen as the deepest search scale, the index list can be built in different ways. Indeed, as a query is made of one spatial index, that points at the desired cell, and an additional depth ( span ), to specify the density of data, the only constraint to maintain the deepest search scale at 26 is the following : where the two left hand side terms are the spatial index size and span value. In these experiments, a first list of index is built using a span of 9 and a second with a span of 10 . As the deepest scale is maintained constant, increasing the span reduces the index list size, but the queried cells contain more data to analyze. The following figures show the mean detection rate for the Switzerland lined-based model with the deepest scale at 26 and span at 9 and 10 . The plots are scaled in the same way as for the user-based experiments : Switzerland : The black curve shows the mean detection rate while the blue area gives the worst and best rates - Span at 9 (left) and 10 (right) One can see that the detection rate on such a model is much better than the user-based or naive approach ones. In a manner of five minutes, for the span set to 10 , the eight differences can be detected and reported. The full detection process took ~5 minutes with span set to 10 and ~8 minutes with the span set to 9 . This shows how the platform can be used by automated processes as an efficient data provider. In addition, as the data are queried by the automated process, the detected primitive geometry is directly available, allowing all sorts of subsequent processes to take place. As the deepest scale was set to 26 , in one of the five measures session, one of the eight differences was not detected (at all). It means that the primitive on which a synthetic variation was introduced is smaller than 30cm and was then not reached by any index. This shows the importance of defining the spatial index and spans according to the processes needs. For example, increasing the deepest scale to 27 would allow reaching primitive down to ~15 cm over the whole Switzerland, and so on. The following figures show the mean detection rate for the Switzerland triangle-based model. In this case, only two measure sessions were made to limit the time spent on this analysis : Switzerland : The black curve shows the mean detection rate while the blue area gives the worst and best rates - Span at 9 (left) and 10 (right) The conclusion remain, but the rate is slower in this case as the model contains much more primitives than the line-based one. In this case, the full detection process took ~15 minutes with span set to 10 and ~20 minutes with the span set to 9 . Again, in one of the two measure session, one difference was not detected due to the size of the primitive. Nevertheless, these results shows how the platform, seen as a process data provider, allows outperforming user-based and classic detection algorithms. Such process-based strategy can be performed in many ways depending on the needs. For example, the index list can be limited to a specific area or set to focus on spread and defined locations (for example at the intersection of the Swiss hectometric grid). The following image gives a simple example of how the detected differences can be leveraged. As the geometry of the differences is known by the process, a summary of the differences can be provided through a simple map : Example of a differences map based on the results of the detection process - Data : SRTM The eight synthetic differences are easily presented allowing a user to analyze them more in detail in the platform interface for example. This map was created detecting the eight differences on the line-based Switzerland model in about 5 minutes with a span set to 10 . Conclusion : First Phase \u00b6 During this first phase, the difference detection algorithm was developed and validated on both line-based and triangle-based data. An efficient algorithm is then implemented in the platform allowing emphasizing differences between models at different temporal positions. The algorithm is able to perform the detection on the fly with good performances allowing the users to dynamically browse the data to detect and analyze the territory evolutions. The performances of the detection algorithm allow the platform to be suitable for automated detection processes, as a data provider, answering large amounts of queries in an efficient and remote manner. Two variations of the difference detection algorithm are implemented. The first version consists in highlighting the primitives that are subject to modifications over a time. This variation is suitable for automated processes that can rely on simple search methods to list the differences. For the users, this first variation can lead to more difficult visual detection of the differences, especially in case the highlighted primitives are small or hidden by others. For this reason, visual markers were added on top of the highlighted primitives in order to be seen from far away, regardless of the primitives size. The measures sessions made during the user-based experiments showed a clear improvement of the detection rate when using the visual markers. This was especially true for triangle-based models, where the primitives bring occlusions. The user-based experiments showed that using the platform interface, a human can significantly outperform the result of a naive automated process operating on the models themselves. The experiments showed that the user is able to efficiently search and find through space and time the evolutions of the territory appearing in the data. Of course, as the model size and complexity increases, the user-driven interface starts to show its limits. In such a case, the process-based experiments showed that automated processes can take over these more complicated searches through methods allowing performing exhaustive detection over wide models in a matter of several minutes. At this point, the developments and validations of the algorithm, and its variations, were conducted on synthetic modifications introduced in models using controlled procedures. The next phase focuses on formal data extracted from land registers. Second Phase : True Variations \u00b6 In this second phase, also dedicated to vector-based models, the focus is set on applying the developed difference detection algorithm on true land register models. Two sets of data are considered in order to address short-term and long-term difference detection. Selected Resources and Models \u00b6 In both cases, short-term and long-term, INTERLIS data are considered. A selection of tables in different topics is performed to extract the most interesting geometries of the land registering. For all models, the following colors are used to distinguish the extracted layers : INTERLIS selected topics and tables colors - Official French and German designations The layers are chosen according to their geometric content. The color assignation is arbitrary and does not correspond to any official colorization standard. Short-Term Difference Detection : Thurgau For the short-term application of the difference detection algorithm, the case of the Thurgau canton is considered. Two set of INTERLIS data are considered that are very close in time, of the order of days. The selected layers are extracted from the source files before to be converted to the WGS84 frame using the EGM95-6 geoid model. The heights are restored using the SRTM topographic model. The following images give an illustration of the considered data : Canton of Thurgau (left) and close view of Frauenfeld (right) - Data : Kanton Thurgau Two INTERLIS models are considered with times 2020-10-13 and 2020-10-17, corresponding to the models gathering time. The following table gives the models size and primitives count : Model Size (UV3) Primitive Count Thurgau 2020-10-13 203.7 Mio 3.8 M-Lines Thurgau 2020-10-17 203.8 Mio 3.8 M-Lines As the two models are very close in time, they are very similar in size and content as the corrections count made during the considered time range is small. Long-Term Difference Detection : Geneva For the long-term difference detection analysis, the Geneva case is selected as the canton of Geneva keeps a copy of each land register model for each month from at least 2009 . This allows to compare INTERLIS models that are further away from each other from a temporal point of view. The selected layers are extracted and converted to the WGS84 coordinates system using the EGM96-6 geoid model. Again, the SRTM model is used to restore the heights. The following images give an illustration of the selected models : Canton of Geneva in 2019-04 (left) and close view of Geneva in 2013-04 (right) - Data : SITG The selected models are not chosen randomly along the time dimension. Models that corresponds to the Geneva LIDAR campaigns are selected as they are used in the next phase. In addition, as the LIDAR campaigns are well spread along the time dimension, the selected models are far away from each other in time, of the order of at least two years. The following table summarize the models size and primitives count : Model Size (UV3) Primitive Count Geneva 2009-10 (MN03) 550.2 Mio 10.3 M-Lines Geneva 2013-04 407.0 Mio 7.6 M-Lines Geneva 2017-04 599.6 Mio 11.2 M-Lines Geneva 2019-04 532.6 Mio 9.9 M-Lines As the temporal gaps between the models are much larger than for the Thurgau models, the size and primitive count show larger variations across the time, indicating that numerous differences should be detected on these data. Models : Statistical Analysis As in the first phase, a statistical analysis of the Thurgau and Geneva models is conducted. The following figures show the line length distribution of the two Thurgau models : Statistical analysis : Primitive size distribution, in meters, for the Thurgau 2020-10-13 (left) and 2020-10-17 (right) As expected, as the models are very similar, the distribution between both models is almost identical. In both cases, the distribution is centered around two meters and is mostly contained within the [0,5] range. The following figures show the same statistical analysis for the Geneva models, more spread along the time dimension : Statistical analysis : Primitive size distribution, in meters, for the Geneva 2009-10 (top-left), 2013-04 (top-right), 2017-04 (bottom-left) and 2019-04 (bottom-right) One can see that the distribution varies more from a time to another. In addition, in comparison with the Thurgau models, the Geneva models tend to have smaller primitive, mostly distributed in the [0,1] range with a narrower distribution. Results and Analysis \u00b6 Short-Term : Thurgau In the case of Thurgau data, the models are only separated in time by a few days. It follows that only a small amount of differences is expected. As an introduction, the following images show the overall situation of the difference detection between the two models. The differences are highlighted by keeping the primitive original color while identities are shown in dark gray to allow context conservation : Overall view of difference detection : Thurgau (right) and Amriswil (left) As expected, as the two models are very close in time, only a limited amount of differences is detected. Such situation allows to have a clear view and understanding of each difference. In order to analyze the results of the difference detection algorithm on real cases , selected differences, using the algorithm itself, are studied more in detail to emphasize the ability of the algorithm to detect and make the difference understandable for the user. As a first example, the case of the Bielackerstrasse in Amriswil is considered and illustrated by the following images : Example of difference detection : Bielackerstrasse in Amriswil - 2020-10-17 (right) and 2020-10-13 (left) as primary time In this case, new buildings are added to the official land register. As the 2020-10-17 is selected as primary, the highlighted elements correspond the footprint of the added buildings. When the 2020-10-13 time is set as primary, as it does not contain the building footprints, the highlighted elements only corresponds to the re-measured elements for land register correction. This illustrates the asymmetry of the difference detection algorithm that only highlight primitives of the primary time. In addition, by keeping the color of the highlighted primitives, the difference detection algorithm allows to immediately see that three layers of the land register have been affected by the modification (German : Einzelobjekte, Flaechenelement Geometrie; Bodenbedeckung, BoFlaeche Geometrie; Einzelobjekte, Linienelement). The following images show the respective situation of the 2020-10-13 and 2020-10_17 models : Situation of Bielackerstrasse in Amriswil - 2020-10-17 (right) and 2020-10-13 (left) This confirms the analysis deduced from the difference detection algorithm that a group of new buildings are added to the land register. In this example, if the inner road was not re-measured, at least on some portion, the difference detection with 2020-10-13 as primary time would have shown noting. To illustrate the asymmetry of the algorithm more clearly, the example of Mammern is considered. On the following image, the result of the difference detection is illustrated with both time chosen successively as primary : Example of difference detection : Mammern - 2020-10-17 (right) and 2020-10-13 (left) as primary time On this specific example, one can see that choosing the 2020-10-17 time as primary, which is the most recent time, nothing is highlighted by the detection algorithm. But when the 2020-10-13 time is set as primary, a specific element appears as highlighted, showing an evolution of the land register. This example illustrates the deletion of a sequence of primitive of the property (German : Liegenschaften, ProjLiegenschaft Geometrie) layer of the land register, which then only appear as the oldest time is set as primary. The following images show both time situation : Situation of Mammern - 2020-10-17 (right) and 2020-10-13 (left) This example shows the opposite situation of the previous one, where elements were deleted from the land register instead of added. As a last example, an in-between situation is selected. The case of the Trungerstrasse in M\u00fcnchwilen is considered and illustrated by the following images showing both time as primary : Example of difference detection : Trungerstrasse in M\u00fcnchwilen - 2020-10-17 (right) and 2020-10-13 (left) as primary time This situation is in-between the two previous one as nothing really appeared and nothing really disappeared from the land register. A modification was made on the situation of this specific property and so, appear no matter which of the two times is selected as primary. The following images show the formal situation of the land register for the two times : Situation of Trungerstrasse in M\u00fcnchwilen - 2020-10-17 (right) and 2020-10-13 (left) One can see that the correction made are around the pointed house, as the access road of the rear delimitation. For this type of situation, the algorithm recover some kind of symmetry, as the selected time as primary does is not relevant to detect the difference. To conclude this short-term difference detection analysis, the efficiency of visual markers is illustrated on the region of Romanshorn and Amriswil on the following images. Both images show the difference detection rendering without and with the visual markers : Illustration of difference detection without (right) and with (left) visual markers - 2020-10-17 as primary time for both images One can see that, for small highlighted primitive, the usage of visual markers eases the differences view for the user. Of course, as the highlighted primitive are big enough, or if the point of view is very close to the model, the efficiency of the visual markers decreases. Long-Term : Geneva Considering the Geneva land register, the compared model are much more spread along the time dimension, leading to a much richer difference model. Starting with the 2019-04 and 2017-04 models, the following images gives an overview of the detected differences on the whole canton : Overall view of difference detection between Geneva 2019-04 and 2017-04 models with 2019-04 as primary On this example, one can see that a much larger amount of differences is detected as the model are separated by two years. As the first observation, one can see that large portions of the model seems to have entirely moved between the two dates. Three of these zones are clearly visible on the images above as all their content is highlighted by the difference detection algorithm : the superior half of the Geneva commune , the Carouge commune and the left half of the Plan-les-Ouates commune , but more can be seen, looking more closely. These zones have been subjected to correction during the time interval separating the two models. These corrections mainly comes from the FINELTRA [1] adjustment used to ensure conversion between the old Swiss coordinates system MN03 and the MN95 current standard. As these corrections operate on each coordinate, the whole area is then modified of the order of a few centimeters. In these condition, the whole area is then highlighted by the difference detection algorithm as illustrated by the following image on the Carouge commune : Closer view of the Carouge 2019-04 and 2017-04 differences with 2019-04 as primary On this closer view, one can see that almost all the primitive of this specific commune have been corrected. Some exceptions remain. It is the case of the train tracks for example, that appear as static between the two models. Looking more closely, one can also observe that some primitive were not affected by the correction. Looking at the areas that have not been corrected through the FINELTRA triangular model, one can see that a lot of modification appear. For example, the following two images gives the differences of the Geneva historical part and the Verbois dam : Closer view of the Historical city (left) and Verbois dam (right) 2019-04 and 2017-04 differences with 2019-04 as primary One can see that, despite very few elements truly changed, a lot of primitives are highlighted as differences. This can be explained by a constant work of correction based on in-situ measurement. Some other factors can also explain these large amount of differences such as scripts used to correct the data to bring them in the expected Swiss standards. In such context, detected real changes of the territory is made much more complicated, as large amounts of detected differences are due to corrections of the model itself, without underlying true modification on the territory. Nevertheless, differences that corresponds to a true territory modification can be found. The following images show an example on the Chemin du Signal in Bernex : Differences on Chemin du Signal in Bernex with 2019-04 (left) and 2017-04 (right) as primary These differences can be detected by the user on the difference model as they appear more clearly due to an accumulation of highlighted primitives. Indeed, in case of simple correction, the highlighted primitive appear more isolated. The following images give the formal situation for the two times : Situation of Chemin du Signal in Bernex in 2019-04 (left) and 2017-04 (right) On this example, one can see that, with both time as primary, the territory evolution can be seen by the user as the highlighted primitives are more consistent. Nevertheless, territory changes are more difficult to list in such a case than in the previous short-term analysis. The following images give two example of visible territory changes in the difference model : La Gradelle (left) and Puplinge (right) 2019-04 and 2017-04 differences with 2019-04 as primary On the previous left image, a clear block of buildings can be seen as more highlighted than the rest of the difference model and correspond to new building. On the right of this block, a smaller one can also be seen that also corresponds to new buildings. On the right images, a clear block of new buildings is also visible, as more highlighted. In such a case, the user has more effort to perform in order to detect the differences that correspond to true changes in the territory, the differences model showing the land register modification in the first place rather than of the proper territory evolution. Considering the 2013-04 model, similar observations apply with stronger effect due to the larger temporal gap. The difference models are dominated by correction made to the model rather than proper territory changes. Comparing the 2017-04 and 2013-04 lead to even more difficult detection of these true modification, as the correction are widely dominating the difference models. The case of the 2009-10 model is made even worse by its coordinates system, as it is expressed in the old MN03 coordinates system. This model is made very difficult to compare with the three others, expressed in the MN95 frame, as all its primitives are highlighted in difference models due to the conversion performed between the MN03 and MN95 frames. Comparing the 2009-10 model with the 2013-04 lead to no primitive detected as identity, leaving only differences. Conclusion : Second Phase \u00b6 Two cases have been addressed in this phase showing each specific interesting application of the difference detection applied on land register data through the INTERLIS format. Indeed, short and long term differences emphasize two different points of view according to the analysis of the land register and its evolution in time. In the first place, the short term application clearly showed how difference detection and their representation opens a new point of view on the evolution of the land register as it allows focusing on clear and well identified modifications. As the compared models are close in time, one is able to produced differences models allowing to clearly see, modification by modification, what happened between the two compare situations, allowing focusing on each evolution to fully understand the modification. It follows that this short-term difference detection can provide a useful approach for the user of the land register that are more interested in the evolution of the model rather than in the model itself. The difference models can provide users a clear a simple view on what to search and to analyze to understand the evolution of such complex models. In some way, the differences on land register models can be seen as an additional layer proposed to the user to allow him to reach information that are not easy to extract from the models themselves. The case of Geneva , illustrating the long-term difference detection case, showed another interesting point of view. In the first place, one has to understand that land register models are complex and living models, not only affected by the transcription of the real-world situation across the time. Indeed, on the Geneva models, a large amount of differences is detected even on a relative short period of time (two years). In addition to the regular updates, following the territory evolution, a large amount of corrections is made to keep the model in the correct reference frame. The Swiss federal system can also add complexity, as all Cantons have to align themselves on a common set of expectations. In such a case, the difference detection turned out to be an interesting tool to understand and follows the corrections made to the model in addition to the regular updates. On the Geneva case, we illustrated that, by detecting it in the difference model, the correction on the coordinates frame on large pieces of the territory. This shows how the difference detection can be seen as a service that can help to keep track of the life of the model by detecting and checking these type of modifications. As a result, difference detection can be a tool for the user of the land register but can also be a tool for the land register authorities themselves. The difference models can be used to check and audit the evolution of the models, helping the required follow-up on the applied correction and updates. Third Phase : Point-Based Models \u00b6 In this third and last phase, the developed algorithm for difference detection on vector models is tested on point-based ones. As mentioned in the introduction, the platform was already implementing logical operators allowing comparing point-based models across time. As illustrated in the introduction, only the AND operator allowed emphasizing differences, but rendering them as missing part of the composite models. It was then difficult for the user to determine and analyze those differences. The goal of this last phase is to determine in which extend the developed algorithm is able to improve the initial results of point-based logical operators and how it can be adapted to provide better detection of differences. Selected Resources and Models \u00b6 Point-Based Models : LIDAR Smaller data-sets are considered as point-based models are usually much larger. The city of Geneva is chosen as an example. Four identical chunks of LIDAR data are considered covering the railway station and its surroundings. The four models correspond to the digitization campaigns of 2005, 2009, 2013 and 2017. The data are converted from LAS to UV3 and brought to WGS84 using the EGM96-5 geoid model. The following images give an overview of the selected models : Point-based data-sets : Geneva LIDAR of 2005 (left) and 2009 (right) - Data : SITG The following table gives a summary of the models sizes and primitive count : Model Size (UV3) Primitive Count Geneva 2005 663.2 Mio 24.8 M-Points Geneva 2009 1.2 Gio 46.7 M-Points Geneva 2013 3.9 Gio 4.2 G-Points Geneva 2017 7.0 Gio 7.5 G-Points The color of the models corresponds to the point classification. In addition, the models have a density that considerably increases with time, from 1 points/m^2 (2005) to 25 points/m^2 (2017). This disparity of density is considered as part of the sampling disparity, leading to a set of data very interesting to analyze and benchmark the difference detection algorithm. Models : Statistical Analysis As for line and triangle-based models, a statistical analysis of the point-based models is performed. The analysis consists in computing an approximation of the nearest neighbor distance distribution of points. The following figure shows the distribution of the 2005 and 2009 models : Statistical analysis : Nearest neighbor distribution approximation of the 2005 (left) and 2009 (right) models and the following figure shows the results for the 2013 and 2017 models : Statistical analysis : Nearest neighbor distribution approximation of the 2013 (left) and 2017 (right) models The nearest neighbor distribution tends to go toward zeros with the year of acquisition, showing that modern models are significantly denser that the older ones, making these models interesting for the difference detection algorithm analysis. Differences Detection Algorithm : Direct Application on Point-Based Models \u00b6 In order to determine the performances of the difference detection algorithm on the selected point-based models, the algorithm is simply applied without any adaptation on the data-sets and the results are analyzed. The following images give an overview of the obtained results comparing the 2005 and 2009 models : Application of the difference detection algorithm on point-based models : Geneva model of 2005 and 2009 with 2005 as primary (left) and inversely (right) - Data SITG One can see that the obtained results are very similar to the results obtained with the previously implemented XOR logical operator. The only differences is that the identical points are shown (in dark gray) along with the highlighted points (showing the differences). The same conclusion applies : the obtained composite model is difficult to read as it is dominated by sampling disparities. One can, by carefully looking at the model, ending up detecting large modifications by searching for highlighted points accumulation. In addition, taking one model or the other as primary for the algorithm does not really help as shown on the images above. The same conclusion applies even when the two compared models comes with a similar point density as the 2013 and 2017 models : Application of the difference detection algorithm on point-based models : Geneva model of 2013 and 2017 with 2013 as primary (left) and inversely (right) - Data SITG One can nevertheless observe that choosing the less dense model as primary leads to results a bit more clear for difference detection, but remaining very hard to interpret for a user, and much more for automated processes. In addition, the performances of the algorithm are very poor as point-based models are much denser in terms of primitives than line or triangle-based models. These reasons lead to the conclusion that the algorithm can not be directly used for point-based models and need a more specific approach. Differences Detection Algorithm : Adaptation for Point-Based Models \u00b6 In order to adapt the difference detection algorithm for point-based models, two aspects have to be addressed : the efficiency of the detection and the reduction of the sampling disparities over-representation, which are both server-side operations. The problem of efficiency can be solved quite easily if the adaptation of the difference detection algorithm goes in the direction of logical operators, for which an efficient methodology is already implemented. Solving the sampling disparity over-representation is more complicated. The adopted solution is inspired from a simple observation : the less deep (density of cells) the queries are, the clearer the obtained representation is. This can be illustrated by the following images showing the 2005 model compared with the 2009 one with depth equal to 7, 6 and 5, from left to right : Example of decreasing query depth on the comparison of 2005 and 2009 models - Data SITG This is expected, as the sampling disparities can only appear at scales corresponding to the nearest neighbor distribution. Nevertheless, as the depth is decreased, the models become less and less dense. The increase of difference readability is then compensated by the lack of density, making the structures more difficult to identify, and then, their subsequent modifications. The goal of the algorithm adaptation is to keep both readability and density. To achieve this goal, the implementation of the previous XOR operator is considered as a base, mostly for its efficiency. As the XOR simply detects if a cell of the space-time discretization at a given time is in a different state as its counterpart at another time, it can be modulated to introduce a scale delay mechanism that only applies detection on low-valued scales, broadcasting their results to their daughter cells. This allows to preserve the density and to perform the detection only on sufficiently shallow scales to avoid sampling disparities to become dominant. The question is how to operate the scale delay according to the scale itself. Indeed, with large points of view, the delay is not necessary as the model is viewed from far away. The necessity of the scale delay appears as the point of view is reduced, and, the more it is reduced, the larger the scale delay needs to be. A scale-attached delay is then defined to associate a specific value for each depth. Results and Experiments \u00b6 The adaptation of the difference detection algorithm for point-based models is analyzed using the selected data-sets. An overview of its result is presented before a more formal analysis is made using difference detection made on line-based official land register data to be compared with the differences on point-based models. Differences Detection : Overview Considering the two first models, from 2005 and 2009 campaigns, the following images shows the results of the initial version of the difference detection algorithm (similar to XOR operator) and its adapted version implementing the scale delay : Differences detection on 2005 and 2009 models with 2005 as primary - Left : without scale delay - Right : with scale delay - Data SITG One can see how scale delay is able to drastically reduce the effect of sampling disparities while comparing two point-based models. The effect is more obvious as the 2009 model is set as primary for difference detection : Differences detection on 2005 and 2009 models with 2009 as primary - Left : without scale delay - Right : with scale delay - Data SITG This improvement gets more clear as the point of view is reduced. The following image shows the initial algorithm and the scale delay algorithm on a specific area of the city with 2005 as primary model : Differences detection on 2005 and 2009 models with 2005 as primary - Left : without scale delay - Right : with scale delay - Data SITG By inverting the model roles and making the 2009 model primary for difference detection lead to similar results : Differences detection on 2005 and 2009 models with 2009 as primary - Left : without scale delay - Right : with scale delay - Data SITG Considering the denser models of 2013 and 2017 campaigns, the results of the scale delay introduction also lead to a better understanding of the differences as shown on the following images : Differences detection on 2013 and 2017 models with scale delay - Left : 2013 as primary - Right : 2017 as primary - Data SITG Nevertheless, one can see that scale delay is not able to get rid entirely of sampling disparities. The right image above, comparing the 2017 model to the 2013 one, shows sampling disparities being highlighted as differences on the wall of the building in the background. This does not affect too much the user readability, but still make the model a bit more complicated to understand. In addition, the models play an important role in the way differences can be detected through classic approach. For example, focusing on a specific building, the obtained highlighted differences : Differences detection on 2013 and 2017 models with scale delay with 2013 (left) and 2017 (right) as primary - Data SITG could lead the user to consider the building wall as a difference. Looking at the formal situation in both 2013 and 2017 models : Structural situation in 2013 (left) and 2017 (right) - Data SITG One can see that the detected difference comes from the missing wall on the 2013, and not from a formal evolution of the building. This example illustrates that sampling disparity is not the only factor that could reduce the readability of the model for the user. Differences Detection : Comparison with Land Register Differences As the algorithm is already tested for land register models, one can use its results on these data in order to put them into perspective of the detected differences on point cloud. As the methodology is not the same for vector-based and point-based models, it is interesting to see the coherence and deviations of both approaches. One important thing to underline, is that difference detection in land register model does not detect changes in the environment directly, but detects the revision of the land register itself, as discussed in the previous phase. Of course, land register models evolve with environment, but come also with a large amount of modifications that only represent corrections of the model and not formal changes in the environment. This reason reinforces the interest to but point-based model difference detection with the land register models ones. In the previous phase, the land register models of Geneva were selected to be the closest to the LIDAR campaigns. It follows, that these models can be directly used here, as each corresponding to the compared point-based model of this phase. As a first example, the following case is studied : Rue de Bourgogne and Rue de Lyon . In this case, looking at the following images giving the situation in 2013-04 and 2017-04 through the LIDAR models, that an industrial building was partially demolished. Structural situation in 2013 (left) and 2017 (right) - Data SITG The following images show the differences computed on both point-based and line-based models : Difference models between 2013 and 2017 of LIDAR (left) and INTERLIS (right), with 2013 as primary - Data SITG One can clearly see that the difference detection on the LIDAR models correctly emphasized a true structural difference between the two times. The situation is much less clear on the land register model. Indeed, as the time separating the two models is quite high, four years in such a case, a large mount of corrections dominates the difference model, leading to a difficult interpretation of the building situation change. The following images give the situation of the land register model in 2013 and 2017 that lead to the difference model above : Land register situation in 2013 (left) and 2017 (right) - Data SITG Looking at the land register models, one can also see that such large scale modification of the building situation does not appear clearly. Indeed, it takes some effort to detect minor changes on the two models, without leading to a clear indication of the modification. This shows how the LIDAR and its differences can help to detect and analyze differences in complement to the land register itself. Considering the second example, Avenue de France and Avenue Blanc , the following images give the structural situation of the two times as capture by the LIDAR campaigns : Structural situation in 2013 (left) and 2017 (right) - Data SITG One can clearly see the destruction of the two 2013 buildings replaced by a parking lot in 2017 . The detected differences on the LIDAR and land register models are presented on the following images : Difference models between 2013 and 2017 of LIDAR (left) and INTERLIS (right), with 2013 as primary - Data SITG Again, despite the differences are clearly and correctly highlighted on the LIDAR differences model, the situation remains unclear on the differences model of the land register. Again, one can observe that the land register was highly corrected between the two dates, leading to difficulties to understand the modification and its nature. Looking at the land register respective models : Land register situation in 2013 (left) and 2017 (right) - Data SITG the modification appears a bit more clearly. One can clearly see the disappearance of the two 2013 buildings in the land register replaced by a big empty area. Again, the difference detection on LIDAR seems clearly more relevant to detect and analyze structural differences than the land register itself. An interesting example is provided by the situation just east of the Basilique Notre-Dame . The two situations as captured by the LIDAR campaigns are presented on the following images : Structural situation in 2013 (left) and 2017 (right) - Data SITG One can observe two structure mounted on top of two buildings roof in the 2013 situation. These structures are used to ease the work that has to be performed on the roofs. These structures are no more present in the 2017 situation. The following images give the difference detection models for the LIDAR and land register : Difference models between 2013 and 2017 of LIDAR (left) and INTERLIS (right), with 2013 as primary - Data SITG In such a case, as the structural modification between 2013 and 2017 occurs on top of the buildings, their footprints is not affected and the differences have no chance to appear in the land register models, even looking at them individually as in the following images : Land register situation in 2013 (left) and 2017 (right) - Data SITG This is another example where the LIDAR difference detection lead to more and clearer information on the structural modification that appear on Geneva between the two times. Conclusion : Third Phase \u00b6 The main element of this third phase conclusion is that difference detection on point-based models is less straightforward than for other models. Indeed, applied naively, the algorithm is dominated by the sampling disparities of the compared models. This illustrate that point-based models, being a close mirror of the true territory state, have a large information density that is more difficult to reach, especially from their evolution point of view. Nevertheless, we showed that the algorithm can be adapted, with relatively simple adjustments, to perform well on point-based models difference detection problem. The implemented algorithm is able to track and represent the differences appearing between the models in a useful and comprehensive way for users. The proposed example showed that the differences models are able to guide the user toward interesting structural changes in the territory, with a clear view of the third dimension. Of course, the highlighted differences in point-based models are more complex and required a trained user that is able to interpret correctly the detail of the highlighted part of the model. The trees are a good example. As the tree re-grow each year, they will always appear as a differences in the compared models. A user only interested in building changes has to be aware of that and be able to separate the relevant differences from the others. Following the comparison between LIDAR and land register ( INTERLIS ) differences models, a very surprising conclusion appear. In the first place, one could stand that land register is the proper way of detected changes that can be then analyzed more in detail in point-based differences models. In turns out that to opposite is true. Several reason explain this surprising situation. In the first place, LIDAR are available only with large temporal gaps between them, at least two/three years. This allows the land register models to be filled with large amount of updates and correction, leading the differences model on this temporal gap to be filled with much more than structural modification. In addition, the LIDAR models come with the third dimension where the land register models are flat. The third dimension comes with large amount of differences that can not be seen in the land register. To some extend, the land register, and its evolution, is the reflect of the way the territory is surveyed, not the reflect of the formal territory evolution. In the opposite, as LIDAR models are a structural snapshot of a territory situation, the analyze of their differences across the time lead to a better tracking of the formal modification of the real world . Conclusion \u00b6 First Phase \u00b6 In the first phase, the difference detection algorithm was implemented for vector models and tested using synthetic differences on selected models. The results showed the interest of the obtained differences models to emphasize evolution of models from both user and process points of view. It was demonstrated that the information between models exists and can be extracted and represented in a relevant way for both users and processes. Second Phase \u00b6 In the second phase, the difference detection algorithm was tested on the Swiss land register models on which the results obtained during the first phase were confirmed. The differences models are able to provide both user and process a clear and understandable view of the modification brought to the models. In addition, through the short and long-term perspectives, it was possible to demonstrate how the difference detection algorithm is able to provide different points of view on the model evolution. From a short-term perspective, the differences models are able to provide a clear and individual view of the modification while the long-term perspective allows to see the large scale evolution and transformation of the models. It follows that the difference models can be used as a tool for various actors using or working with the land register models. Third Phase \u00b6 In the third phase, the difference detection algorithm, developed on vector models, was applied on point-based models, showing that a direct application on these models lead to the same issue as the logical operators : the differences models are dominated by sampling disparities, making them complicated to read. The solution of scale delay brought to the algorithm allowed to produce much clearer differences models for point-based data, allowing to generalize the difference detection on any models. In addition to these results, the comparison of difference models on land register and on their corresponding LIDAR point-based models showed an interesting result : for structural changes, the point-based models lead to much more interesting results through the highlighted differences. Indeed, as land register models, considered long term perspective, are dominated by a large amount of corrections and adjustments in addition to territory evolution updates, making the structural changes not easy to detect and understand. The differences models are more clear with point-based models form this point of view. In addition, as point-based models, such as LIDAR , come with the third dimension, a large amount of structural differences can only be seen through such data as many structural changes are made along the third dimension. It then follows that difference detection applied to point-based models offers a very interesting point of view for the survey of territory structural changes. Synthesis \u00b6 As a synthesis, it is clear that models are carrying a large amount of richness themselves, that is already a challenge to exploit, but it is also clear that a large amount of information can be found between the versions of the models. The difference detection algorithm brings a first tool that demonstrate the ability to reach and start to exploit these informations. More than the content of the models itself, the understanding of the evolution of this content is a major topic especially in the field of geodata as they represent, transcript, the evolution of the surveyed territory. It then appears clear that being able to reach and exploit the information contained in-between the models is a major advantage as it allows understanding what are these models, that is four dimensional objects. Perspectives \u00b6 Many perspectives are opened following the implementation and analysis of the difference detection. Several perspectives, mostly technical, are presented here as a final section. In the first place, as raster are entering the set of data that can be injected in the platform, evolution of the difference detection could be applied to the platform, taking advantage of the evolution of machine learning. The possibility of detected differences in images could lead to very interesting perspective through the data communication features of the platform. Another perspective could be to allow the platform to separate the data into formal layers, the separation being only currently ensure by type and times. Splitting data into layers would allow applying difference detection in a much more controlled manner, leading to difference models focused on very specific elements of the model temporal evolution. The addition of layer could also be the starting point to the notion of data convolution micro language . Currently, data communication and difference detection only apply through the specification of two different and parallel navigation time. The users, or processes, have to specify each of the two time position in order to obtain the mixed of differences models they need. An interesting evolution would be to replace these two navigation time by a small and simple micro language allowing the user to compare more than two times in a more complex manner. This could also benefit from data separation through layer. Such micro language could allow to compare two, three or more models, or layers, and would also open the access the mixed models of differences models such as comparing the difference detection between point-based and vector-based models, which would then be a comparison of a comparison. Reproduction Resources \u00b6 To reproduce the presented experiments, the STDL 4D framework has to be used and can be found here : STDL 4D framework (eratosthene-suite), STDL You can follow the instructions on the README to both compile and use the framework. Only part of the considered datasets are publicly available. For the OpenStreetMap datasets, you can download them from the following source : Shapefile layers, OpenStreetMap For the Swiss 3D buildings model, you can contact swisstopo : Shapefile 3D buildings, swisstopo For the land register datasets of Geneva and Thurgau , you can contact the SITG and the Thurgau Kanton : INTERLIS land register, Thurgau Kanton INTERLIS land register, SITG (Geneva) The point-based models of Geneva can be downloaded from the SITG online extractor : LAS MNS, SITG (Geneva) To extract and convert the data from planimetric shapefiles , the following code is used : Shapefile CSV export to UV3 (csv-wkt-to-uv3), STDL where the README gives all the information needed. In case of shapefile containing 3D models, please ask the STDL for advice and tools. To extract and convert the data from INTERLIS and LAS , the following codes are used : INTERLIS to UV3 (dalai-suite), STDL/EPFL LAS to UV3 (dalai-suite), STDL/EPFL where the README gives all the information needed. For the 3D geographical coordinates conversion and heights restoration, we used two STDL internal tools. You can contact the STDL to obtain the tools and support in this direction : ptolemee-suite : 3D coordinate conversion tool (EPSG:2056 to WGS84) height-from-geotiff : Restoring geographical heights using topographic GeoTIFF ( SRTM ) You can contact STDL for any question regarding the reproduction of the presented results. Auxiliary Developments & Corrections \u00b6 In addition to the main developments made, some additional scripts and other corrections have been made to solve auxiliary problems or to improve the code according to the developed features during this task. The auxiliary developments are summarized here : Correction of socket read function to improve server-client connectivity. Creation of scripts that allows to insert synthetic modifications (random displacements on the vertex coordinates) on UV3 models. Creation of a script to convert CSV export from shapefile to UV3 format. The script code is available here . Adding temporary addresses (space-time index) exportation in platform 3D interface. Correction of the cell enumeration process in platform 3D interface (wrong depth limit implementation). Creation of a script allowing segmenting UV3 model according to geographical bounding box. Creation of C codes to perform statistical analysis of the point, line and triangle-based models : computation of edge size and nearest neighbor distributions. Creation of a C code allowing enumerating non-empty cell index over the Switzerland models injected in the platform. Creation of a C code allowing to automate the difference detection based on an index list and by searching in the data queried from the platform. Developments of various scripts for plots and figures creations. References \u00b6 [1] REFRAME, SwissTopo, https://www.swisstopo.admin.ch/de/karten-daten-online/calculation-services.html","title":"AUTOMATIC DETECTION OF CHANGES IN THE ENVIRONMENT"},{"location":"TASK-DIFF/#automatic-detection-of-changes-in-the-environment","text":"Nils Hamel (UNIGE) Project scheduled in the STDL research roadmap - TASK-DIFF September 2020 to November 2020 - Published on December 7, 2020 Abstract : Developed at EPFL with the collaboration of Cadastre Suisse to handle large scale geographical models of different nature, the STDL 4D platform offers a robust and efficient indexation methodology allowing to manage storage and access to large-scale models. In addition to spatial indexation, the platform also includes time as part of the indexation, allowing any area to be described by models in both spatial and temporal dimensions. In this development project, the notion of model temporal derivative is explored and proof-of-concepts are implemented in the platform. The goal is to demonstrate that, in addition to their formal content, models coming with different temporal versions can be derived along the time dimension to compute difference models. Such proof-of-concept is developed for both point cloud and vectorial models, demonstrating that the indexation formalism of the platform is able to ease considerably the computation of difference models. This research project demonstrates that the time dimension can be fully exploited in order to access the data it holds.","title":"AUTOMATIC DETECTION OF CHANGES IN THE ENVIRONMENT"},{"location":"TASK-DIFF/#task-context-difference-detection","text":"As the implemented indexation formalism is based on equivalences classes defined on space and time, a natural discretization along all the four dimensions is obtained. In the field of difference detection, it allowed implementing simple logical operators on the four-dimensional space. The OR , AND and XOR operators were then implemented allowing the platform to compute, in real time , convolutions to compare models with each others across the time. The implementation of these operators was simple due to the natural spatio-temporal discretization obtained from the indexation formalism. Nevertheless, two major drawbacks appeared : the first one is that such operators only works for point-based models. Having the opportunity to compute and render differences and similarities between any type of data is not possible with such formal operators. The second drawback comes from the nature of the point-based capturing devices. Indeed, taking the example of a building, even without any change to its structure, two digitization campaigns can lead to disparities only due to measures sampling. The XOR operator is the natural choice to detect and render differences, but this operator is very sensitive to sampling disparities. Computing the XOR convolution between two point-based models leads the rendering to be dominated by sampling variations rather than the desired structural differences. This drawback was partially solved by considering the AND operator. Indeed, the AND operator allows to only shows constant structural elements from two different positions in time and is insensitive to sampling disparities. As shown on the following images, the AND operator shows differences as black spots (missing parts) : AND convolution between two LIDAR models : Geneva 2005 and 2009 - Data : SITG As one can see, AND convolutions allow detecting, through the black spots, large area of structural changes between the two times and also, with more care, allow guessing smaller differences. Nevertheless, reading and interpreting such representation remains complex for users. The goal of this task is then to tackle these two drawbacks, allowing the platform to detect changes not only for point-based models but also for vector-based models and to implement a variation of the XOR operator for point-based models allowing to efficiently highlight the structural evolution. The task consists then in the implementation, testing and validation of a difference detection algorithm suitable for any type of model and to conduct a formal analysis on the best rendering techniques.","title":"Task Context : Difference Detection"},{"location":"TASK-DIFF/#methodology","text":"A step by step methodology is defined to address the problem of difference detection in the platform. In a first phase, the algorithm will be developed and validated on vector-based models as follows : Obtaining a large scale vector-based model on which synthetic variation are introduced Development of the algorithm using the synthetic variations model Testing and validation of the algorithm (using the known synthetic variations) First conclusion In a second phase, true land register data will be used to formally detect real evolutions of the territory : Obtaining true land register vector-based models (INTERLIS) at different times Analysis of the difference detection algorithm on true land register vector-based models Second conclusion In a third phase, the algorithm will be validated and adapted to work on point-based models : Obtaining true land register point-based models (LAS) at different position in time Verifying the performances of the vector-based detection algorithm on point-based data Adaptation of the algorithm for point-based models Analysis of the difference detection algorithm on true land register point-based models Comparison of the detected differences on point-based models and on their corresponding land register vector-based models (INTERLIS) Third conclusion In addition, the development of difference detection algorithm has to be conducted keeping in mind the possible future evolutions of the platform such as addition of layers (separation of data), implementation of a multi-scale approach of the time dimension and addition of raster data in the platform.","title":"Methodology"},{"location":"TASK-DIFF/#first-phase-synthetic-variations","text":"In order to implements the vector-based difference detection algorithm, sets of data are considered as base on which synthetic differences are applied to simulate the evolution of the territory. This approach allows focusing on well controlled data to formally benchmark the results of the implemented algorithm. Experiments are conducted using these data to formally evaluate the performance of the developed algorithm.","title":"First Phase : Synthetic Variations"},{"location":"TASK-DIFF/#selected-resources-and-models","text":"","title":"Selected Resources and Models"},{"location":"TASK-DIFF/#implementation-of-the-algorithm","text":"In order to compare two models at two different positions in time to detect differences, the solution is of course to search for each primitive of the primary time if it has a corresponding one in the secondary time. In such case, the primitives can be concluded as static in time and only the primitives that have no correspondence will be highlighted as differences. A first approach was initially tested : a vertex-based comparison. As every primitive (points, lines and triangles) is supported by vertexes, it can be seen as a common denominator on which comparison can take place. Unfortunately, it is not a relevant approach as it leads to an asymmetric detection algorithm. To illustrate the issue, the following image shows the situation of a group of line-based primitives at two different times with an evolution on one of the primitive vertex : Asymmetric approach : The variation is detected only when comparing backward in time When the comparison occurs between the second time and the first one, the modified vertex correspondence is not found, and the vertex can be highlighted as a difference. The asymmetry appears as the first time is compared to the second one. In this case, despite the primitive vertex changed, the vertex-based approach is able to find another vertex, part of another primitive, and interprets it as a vertex identity, leading the modified primitive to be considered as static. In order to obtain a fully symmetric algorithm, that does not depend on the way models are compared in times, a primitive-attached approach is considered. The implemented algorithm then treats the correspondence problem from the whole primitive point of view, by checking that the whole primitive can be found in the other model to which it is compared to. This allows to highlight any primitive showing a modification, regardless of the way models are compared and the nature of the modification. In addition to highlighting the primitives that changed through time, the implemented algorithm also renders the primitives that have not changed. The primitives are then shown by modulating their color to emphasize the modifications by keeping their original color for the modified one, while the static primitives are shown in dark gray. This allows to not only show the modifications but also to keep the context of the modifications, helping the user to fully understand the nature of the territory evolution. In addition to color modulation, a variation of difference rendering is analyzed. In addition to color modulation, a visual and artificial marker is added to ease their search. The visual marker is a simple line emanating from the primitive and goes straight up with a size of 512 meters. Such markers are introduced to ease the detection of small primitives that can be difficult to spot according to large point of views. Additional developments were required for triangle-based models : indeed, such models need to be subjected to a light source during rendering for the user to understand the model (face shading). The previously implemented lighting model is then modified to take into account color modulation in order to correctly render the triangle that are highlighted. Moreover, the lighting model was modified to light both face of the triangles in order to light them regardless of the point of view. In addition, as mesh models are made of triangles, primitives can hide themselves. It can then be difficult for the user to spot the highlighted primitives as they can be hidden by others. An option was added to the rendering client allowing the user to ask the rendering of triangles as line-loops or points in order to make them transparent. Finally, an option allowing the user to enable or disable the render face culling was added for him to be able to see the primitive from backward.","title":"Implementation of the Algorithm"},{"location":"TASK-DIFF/#results-and-experiments","text":"With the implemented algorithm, a series of experiments are conducted in order to validate its results and to analyze the efficiency of the difference detection and rendering from a user point of view. In addition, experiments are also conducted to quantify the efficiency of the difference detection for automated processes.","title":"Results and Experiments"},{"location":"TASK-DIFF/#conclusion-first-phase","text":"During this first phase, the difference detection algorithm was developed and validated on both line-based and triangle-based data. An efficient algorithm is then implemented in the platform allowing emphasizing differences between models at different temporal positions. The algorithm is able to perform the detection on the fly with good performances allowing the users to dynamically browse the data to detect and analyze the territory evolutions. The performances of the detection algorithm allow the platform to be suitable for automated detection processes, as a data provider, answering large amounts of queries in an efficient and remote manner. Two variations of the difference detection algorithm are implemented. The first version consists in highlighting the primitives that are subject to modifications over a time. This variation is suitable for automated processes that can rely on simple search methods to list the differences. For the users, this first variation can lead to more difficult visual detection of the differences, especially in case the highlighted primitives are small or hidden by others. For this reason, visual markers were added on top of the highlighted primitives in order to be seen from far away, regardless of the primitives size. The measures sessions made during the user-based experiments showed a clear improvement of the detection rate when using the visual markers. This was especially true for triangle-based models, where the primitives bring occlusions. The user-based experiments showed that using the platform interface, a human can significantly outperform the result of a naive automated process operating on the models themselves. The experiments showed that the user is able to efficiently search and find through space and time the evolutions of the territory appearing in the data. Of course, as the model size and complexity increases, the user-driven interface starts to show its limits. In such a case, the process-based experiments showed that automated processes can take over these more complicated searches through methods allowing performing exhaustive detection over wide models in a matter of several minutes. At this point, the developments and validations of the algorithm, and its variations, were conducted on synthetic modifications introduced in models using controlled procedures. The next phase focuses on formal data extracted from land registers.","title":"Conclusion : First Phase"},{"location":"TASK-DIFF/#second-phase-true-variations","text":"In this second phase, also dedicated to vector-based models, the focus is set on applying the developed difference detection algorithm on true land register models. Two sets of data are considered in order to address short-term and long-term difference detection.","title":"Second Phase : True Variations"},{"location":"TASK-DIFF/#selected-resources-and-models_1","text":"In both cases, short-term and long-term, INTERLIS data are considered. A selection of tables in different topics is performed to extract the most interesting geometries of the land registering. For all models, the following colors are used to distinguish the extracted layers : INTERLIS selected topics and tables colors - Official French and German designations The layers are chosen according to their geometric content. The color assignation is arbitrary and does not correspond to any official colorization standard.","title":"Selected Resources and Models"},{"location":"TASK-DIFF/#results-and-analysis","text":"","title":"Results and Analysis"},{"location":"TASK-DIFF/#conclusion-second-phase","text":"Two cases have been addressed in this phase showing each specific interesting application of the difference detection applied on land register data through the INTERLIS format. Indeed, short and long term differences emphasize two different points of view according to the analysis of the land register and its evolution in time. In the first place, the short term application clearly showed how difference detection and their representation opens a new point of view on the evolution of the land register as it allows focusing on clear and well identified modifications. As the compared models are close in time, one is able to produced differences models allowing to clearly see, modification by modification, what happened between the two compare situations, allowing focusing on each evolution to fully understand the modification. It follows that this short-term difference detection can provide a useful approach for the user of the land register that are more interested in the evolution of the model rather than in the model itself. The difference models can provide users a clear a simple view on what to search and to analyze to understand the evolution of such complex models. In some way, the differences on land register models can be seen as an additional layer proposed to the user to allow him to reach information that are not easy to extract from the models themselves. The case of Geneva , illustrating the long-term difference detection case, showed another interesting point of view. In the first place, one has to understand that land register models are complex and living models, not only affected by the transcription of the real-world situation across the time. Indeed, on the Geneva models, a large amount of differences is detected even on a relative short period of time (two years). In addition to the regular updates, following the territory evolution, a large amount of corrections is made to keep the model in the correct reference frame. The Swiss federal system can also add complexity, as all Cantons have to align themselves on a common set of expectations. In such a case, the difference detection turned out to be an interesting tool to understand and follows the corrections made to the model in addition to the regular updates. On the Geneva case, we illustrated that, by detecting it in the difference model, the correction on the coordinates frame on large pieces of the territory. This shows how the difference detection can be seen as a service that can help to keep track of the life of the model by detecting and checking these type of modifications. As a result, difference detection can be a tool for the user of the land register but can also be a tool for the land register authorities themselves. The difference models can be used to check and audit the evolution of the models, helping the required follow-up on the applied correction and updates.","title":"Conclusion : Second Phase"},{"location":"TASK-DIFF/#third-phase-point-based-models","text":"In this third and last phase, the developed algorithm for difference detection on vector models is tested on point-based ones. As mentioned in the introduction, the platform was already implementing logical operators allowing comparing point-based models across time. As illustrated in the introduction, only the AND operator allowed emphasizing differences, but rendering them as missing part of the composite models. It was then difficult for the user to determine and analyze those differences. The goal of this last phase is to determine in which extend the developed algorithm is able to improve the initial results of point-based logical operators and how it can be adapted to provide better detection of differences.","title":"Third Phase : Point-Based Models"},{"location":"TASK-DIFF/#selected-resources-and-models_2","text":"","title":"Selected Resources and Models"},{"location":"TASK-DIFF/#differences-detection-algorithm-direct-application-on-point-based-models","text":"In order to determine the performances of the difference detection algorithm on the selected point-based models, the algorithm is simply applied without any adaptation on the data-sets and the results are analyzed. The following images give an overview of the obtained results comparing the 2005 and 2009 models : Application of the difference detection algorithm on point-based models : Geneva model of 2005 and 2009 with 2005 as primary (left) and inversely (right) - Data SITG One can see that the obtained results are very similar to the results obtained with the previously implemented XOR logical operator. The only differences is that the identical points are shown (in dark gray) along with the highlighted points (showing the differences). The same conclusion applies : the obtained composite model is difficult to read as it is dominated by sampling disparities. One can, by carefully looking at the model, ending up detecting large modifications by searching for highlighted points accumulation. In addition, taking one model or the other as primary for the algorithm does not really help as shown on the images above. The same conclusion applies even when the two compared models comes with a similar point density as the 2013 and 2017 models : Application of the difference detection algorithm on point-based models : Geneva model of 2013 and 2017 with 2013 as primary (left) and inversely (right) - Data SITG One can nevertheless observe that choosing the less dense model as primary leads to results a bit more clear for difference detection, but remaining very hard to interpret for a user, and much more for automated processes. In addition, the performances of the algorithm are very poor as point-based models are much denser in terms of primitives than line or triangle-based models. These reasons lead to the conclusion that the algorithm can not be directly used for point-based models and need a more specific approach.","title":"Differences Detection Algorithm : Direct Application on Point-Based Models"},{"location":"TASK-DIFF/#differences-detection-algorithm-adaptation-for-point-based-models","text":"In order to adapt the difference detection algorithm for point-based models, two aspects have to be addressed : the efficiency of the detection and the reduction of the sampling disparities over-representation, which are both server-side operations. The problem of efficiency can be solved quite easily if the adaptation of the difference detection algorithm goes in the direction of logical operators, for which an efficient methodology is already implemented. Solving the sampling disparity over-representation is more complicated. The adopted solution is inspired from a simple observation : the less deep (density of cells) the queries are, the clearer the obtained representation is. This can be illustrated by the following images showing the 2005 model compared with the 2009 one with depth equal to 7, 6 and 5, from left to right : Example of decreasing query depth on the comparison of 2005 and 2009 models - Data SITG This is expected, as the sampling disparities can only appear at scales corresponding to the nearest neighbor distribution. Nevertheless, as the depth is decreased, the models become less and less dense. The increase of difference readability is then compensated by the lack of density, making the structures more difficult to identify, and then, their subsequent modifications. The goal of the algorithm adaptation is to keep both readability and density. To achieve this goal, the implementation of the previous XOR operator is considered as a base, mostly for its efficiency. As the XOR simply detects if a cell of the space-time discretization at a given time is in a different state as its counterpart at another time, it can be modulated to introduce a scale delay mechanism that only applies detection on low-valued scales, broadcasting their results to their daughter cells. This allows to preserve the density and to perform the detection only on sufficiently shallow scales to avoid sampling disparities to become dominant. The question is how to operate the scale delay according to the scale itself. Indeed, with large points of view, the delay is not necessary as the model is viewed from far away. The necessity of the scale delay appears as the point of view is reduced, and, the more it is reduced, the larger the scale delay needs to be. A scale-attached delay is then defined to associate a specific value for each depth.","title":"Differences Detection Algorithm : Adaptation for Point-Based Models"},{"location":"TASK-DIFF/#results-and-experiments_1","text":"The adaptation of the difference detection algorithm for point-based models is analyzed using the selected data-sets. An overview of its result is presented before a more formal analysis is made using difference detection made on line-based official land register data to be compared with the differences on point-based models.","title":"Results and Experiments"},{"location":"TASK-DIFF/#conclusion-third-phase","text":"The main element of this third phase conclusion is that difference detection on point-based models is less straightforward than for other models. Indeed, applied naively, the algorithm is dominated by the sampling disparities of the compared models. This illustrate that point-based models, being a close mirror of the true territory state, have a large information density that is more difficult to reach, especially from their evolution point of view. Nevertheless, we showed that the algorithm can be adapted, with relatively simple adjustments, to perform well on point-based models difference detection problem. The implemented algorithm is able to track and represent the differences appearing between the models in a useful and comprehensive way for users. The proposed example showed that the differences models are able to guide the user toward interesting structural changes in the territory, with a clear view of the third dimension. Of course, the highlighted differences in point-based models are more complex and required a trained user that is able to interpret correctly the detail of the highlighted part of the model. The trees are a good example. As the tree re-grow each year, they will always appear as a differences in the compared models. A user only interested in building changes has to be aware of that and be able to separate the relevant differences from the others. Following the comparison between LIDAR and land register ( INTERLIS ) differences models, a very surprising conclusion appear. In the first place, one could stand that land register is the proper way of detected changes that can be then analyzed more in detail in point-based differences models. In turns out that to opposite is true. Several reason explain this surprising situation. In the first place, LIDAR are available only with large temporal gaps between them, at least two/three years. This allows the land register models to be filled with large amount of updates and correction, leading the differences model on this temporal gap to be filled with much more than structural modification. In addition, the LIDAR models come with the third dimension where the land register models are flat. The third dimension comes with large amount of differences that can not be seen in the land register. To some extend, the land register, and its evolution, is the reflect of the way the territory is surveyed, not the reflect of the formal territory evolution. In the opposite, as LIDAR models are a structural snapshot of a territory situation, the analyze of their differences across the time lead to a better tracking of the formal modification of the real world .","title":"Conclusion : Third Phase"},{"location":"TASK-DIFF/#conclusion","text":"","title":"Conclusion"},{"location":"TASK-DIFF/#first-phase","text":"In the first phase, the difference detection algorithm was implemented for vector models and tested using synthetic differences on selected models. The results showed the interest of the obtained differences models to emphasize evolution of models from both user and process points of view. It was demonstrated that the information between models exists and can be extracted and represented in a relevant way for both users and processes.","title":"First Phase"},{"location":"TASK-DIFF/#second-phase","text":"In the second phase, the difference detection algorithm was tested on the Swiss land register models on which the results obtained during the first phase were confirmed. The differences models are able to provide both user and process a clear and understandable view of the modification brought to the models. In addition, through the short and long-term perspectives, it was possible to demonstrate how the difference detection algorithm is able to provide different points of view on the model evolution. From a short-term perspective, the differences models are able to provide a clear and individual view of the modification while the long-term perspective allows to see the large scale evolution and transformation of the models. It follows that the difference models can be used as a tool for various actors using or working with the land register models.","title":"Second Phase"},{"location":"TASK-DIFF/#third-phase","text":"In the third phase, the difference detection algorithm, developed on vector models, was applied on point-based models, showing that a direct application on these models lead to the same issue as the logical operators : the differences models are dominated by sampling disparities, making them complicated to read. The solution of scale delay brought to the algorithm allowed to produce much clearer differences models for point-based data, allowing to generalize the difference detection on any models. In addition to these results, the comparison of difference models on land register and on their corresponding LIDAR point-based models showed an interesting result : for structural changes, the point-based models lead to much more interesting results through the highlighted differences. Indeed, as land register models, considered long term perspective, are dominated by a large amount of corrections and adjustments in addition to territory evolution updates, making the structural changes not easy to detect and understand. The differences models are more clear with point-based models form this point of view. In addition, as point-based models, such as LIDAR , come with the third dimension, a large amount of structural differences can only be seen through such data as many structural changes are made along the third dimension. It then follows that difference detection applied to point-based models offers a very interesting point of view for the survey of territory structural changes.","title":"Third Phase"},{"location":"TASK-DIFF/#synthesis","text":"As a synthesis, it is clear that models are carrying a large amount of richness themselves, that is already a challenge to exploit, but it is also clear that a large amount of information can be found between the versions of the models. The difference detection algorithm brings a first tool that demonstrate the ability to reach and start to exploit these informations. More than the content of the models itself, the understanding of the evolution of this content is a major topic especially in the field of geodata as they represent, transcript, the evolution of the surveyed territory. It then appears clear that being able to reach and exploit the information contained in-between the models is a major advantage as it allows understanding what are these models, that is four dimensional objects.","title":"Synthesis"},{"location":"TASK-DIFF/#perspectives","text":"Many perspectives are opened following the implementation and analysis of the difference detection. Several perspectives, mostly technical, are presented here as a final section. In the first place, as raster are entering the set of data that can be injected in the platform, evolution of the difference detection could be applied to the platform, taking advantage of the evolution of machine learning. The possibility of detected differences in images could lead to very interesting perspective through the data communication features of the platform. Another perspective could be to allow the platform to separate the data into formal layers, the separation being only currently ensure by type and times. Splitting data into layers would allow applying difference detection in a much more controlled manner, leading to difference models focused on very specific elements of the model temporal evolution. The addition of layer could also be the starting point to the notion of data convolution micro language . Currently, data communication and difference detection only apply through the specification of two different and parallel navigation time. The users, or processes, have to specify each of the two time position in order to obtain the mixed of differences models they need. An interesting evolution would be to replace these two navigation time by a small and simple micro language allowing the user to compare more than two times in a more complex manner. This could also benefit from data separation through layer. Such micro language could allow to compare two, three or more models, or layers, and would also open the access the mixed models of differences models such as comparing the difference detection between point-based and vector-based models, which would then be a comparison of a comparison.","title":"Perspectives"},{"location":"TASK-DIFF/#reproduction-resources","text":"To reproduce the presented experiments, the STDL 4D framework has to be used and can be found here : STDL 4D framework (eratosthene-suite), STDL You can follow the instructions on the README to both compile and use the framework. Only part of the considered datasets are publicly available. For the OpenStreetMap datasets, you can download them from the following source : Shapefile layers, OpenStreetMap For the Swiss 3D buildings model, you can contact swisstopo : Shapefile 3D buildings, swisstopo For the land register datasets of Geneva and Thurgau , you can contact the SITG and the Thurgau Kanton : INTERLIS land register, Thurgau Kanton INTERLIS land register, SITG (Geneva) The point-based models of Geneva can be downloaded from the SITG online extractor : LAS MNS, SITG (Geneva) To extract and convert the data from planimetric shapefiles , the following code is used : Shapefile CSV export to UV3 (csv-wkt-to-uv3), STDL where the README gives all the information needed. In case of shapefile containing 3D models, please ask the STDL for advice and tools. To extract and convert the data from INTERLIS and LAS , the following codes are used : INTERLIS to UV3 (dalai-suite), STDL/EPFL LAS to UV3 (dalai-suite), STDL/EPFL where the README gives all the information needed. For the 3D geographical coordinates conversion and heights restoration, we used two STDL internal tools. You can contact the STDL to obtain the tools and support in this direction : ptolemee-suite : 3D coordinate conversion tool (EPSG:2056 to WGS84) height-from-geotiff : Restoring geographical heights using topographic GeoTIFF ( SRTM ) You can contact STDL for any question regarding the reproduction of the presented results.","title":"Reproduction Resources"},{"location":"TASK-DIFF/#auxiliary-developments-corrections","text":"In addition to the main developments made, some additional scripts and other corrections have been made to solve auxiliary problems or to improve the code according to the developed features during this task. The auxiliary developments are summarized here : Correction of socket read function to improve server-client connectivity. Creation of scripts that allows to insert synthetic modifications (random displacements on the vertex coordinates) on UV3 models. Creation of a script to convert CSV export from shapefile to UV3 format. The script code is available here . Adding temporary addresses (space-time index) exportation in platform 3D interface. Correction of the cell enumeration process in platform 3D interface (wrong depth limit implementation). Creation of a script allowing segmenting UV3 model according to geographical bounding box. Creation of C codes to perform statistical analysis of the point, line and triangle-based models : computation of edge size and nearest neighbor distributions. Creation of a C code allowing enumerating non-empty cell index over the Switzerland models injected in the platform. Creation of a C code allowing to automate the difference detection based on an index list and by searching in the data queried from the platform. Developments of various scripts for plots and figures creations.","title":"Auxiliary Developments &amp; Corrections"},{"location":"TASK-DIFF/#references","text":"[1] REFRAME, SwissTopo, https://www.swisstopo.admin.ch/de/karten-daten-online/calculation-services.html","title":"References"},{"location":"TASK-DTRK/","text":"DIFFERENCE MODELS APPLIED ON LAND REGISTER \u00b6 Nils Hamel (UNIGE) - Huriel Reichel (swisstopo) Project scheduled in the STDL research roadmap - TASK-DTRK September 2020 to November 2020 - Published on ? ?, 2021 Abstract : Being able to track modifications in the evolution of geographical datasets is one important aspect in territory management, as a large amount of information can be extracted out of differences models. Differences detection can also be a tool used to assess the evolution of a geographical model through time. In this research project, we apply differences detection on INTERLIS models of the official Swiss land registers in order to emphasize and follow its evolution and to demonstrate that change in reference frames can be detected and assessed. Introduction \u00b6 Land register models are probably to most living of the geographical models as they are constantly updated to offer a rigorous and up-to-date view of the territory. The applied corrections are always the result of a complex process, involving different territory actors, until the decision is made to integrate them into the land register. In addition, land register models comes with an additional constraint linked to political decisions. Indeed, the land register models are the result of a political mission conducted under federal laws making these models of high importance and requiring constant care. We show in this research project how differences detection tool [1] of the STDL 4D framework can be used to emphasize and analyze these corrections along the time dimension. In addition to the constant updates of the models, changes in the reference frame can also lead to large-scale corrections of the land register models. These global corrections are then made even more complex by the federal laws that impose a high degree of correctness and accuracy. In the context of the introduction of the new reference frame DM.flex [2] for the Swiss land register, being able to assess the applied changes on the geographical model appear as an important aspect. Indeed, changing the reference frame for the land register models is a long and complex technical process that can be error prompt. We also show in this research project how the difference detection algorithm can be helpful to assess and verify the performed corrections. Research Project Specifications \u00b6 In this research project, the difference detection algorithm implemented in the STDL 4D framework is applied on INTERLIS data containing the official land register models of different Swiss Canton . As introduced, two main directions are considered for the difference detection algorithm : Demonstrating the ability to extract information in between land register models Demonstrating the ability of difference models to be used as an assessment tool Through the first direction, the difference detection algorithm is presented. Considering the difference models it allows computing, it is shown how such model are able to extract information in between the models in order to emphasize the ability to represent, and then, to verify the evolution of the land register models. The second direction focuses on demonstrating that difference models are a helpful representation of the large-scale corrections that can be applied to land register during reference frame modification and how they can be used as a tool to assess the modifications and to help to fulfil the complex task of the verification of the corrected models. Research Project Data \u00b6 For the first research direction, the land register models of the Thurgau Kanton are considered. They are selected in order to have a small temporal distance allowing to focus on a small amount of well-defined differences : Thurgau Kanton , 2020-10-13 , INTERLIS Thurgau Kanton , 2020-10-17 , INTERLIS For the second direction, which focus on more complex differences, the models of the Canton of Geneva land register are considered with a much larger temporal gap between them : Canton of Geneva , 2009-10 , INTERLIS Canton of Geneva , 2013-04 , INTERLIS Canton of Geneva , 2017-04 , INTERLIS Canton of Geneva , 2019-04 , INTERLIS Difference Models : A Temporal Derivative \u00b6 This first section focuses on short-term differences to show how difference models work and how they are able to represent the modifications extracted out of the two compared models. The following images give an illustration of the considered dataset, which are the land register models of Thurgau Kanton : Illustration of Thurgau Kanton INTERLIS models - Data : Kanton Thurgau The models are made of vector lines, well geo-referenced in the Swiss coordinates frame EPSG:2056 . The models are also made of different layers that are colored differently with the following correspondences : INTERLIS selected topics and tables colors - Official French and German designations These legends are used all along this research project. Considering two temporal versions of this geographical model, separated of a few days, one is able to extract difference models using the 4D framework algorithm. As an example, one can consider this very specific view of the land register, focusing on a few houses : Close view of the Thurgau INTERLIS model in 2020-10-13 (left) and 2020-10-17 (right) - Data : Kanton Thurgau It is clear that most of the close view is identical for the two models, except for a couple of houses that were added to the land register model between these two temporal versions. By applying the difference detection algorithm, one is able to obtain a difference model comparing the two previous models. The following image gives an illustration of the obtained difference models considering the most recent temporal version as reference : Difference model obtained comparing the two temporal versions - Data : Kanton Thrugau One can see how the difference algorithm is able to emphasize the differences and to represent them in a human-readable third model. The algorithm also displays the identical parts in dark gray to offer the context of the differences to the operator. Of course, in such close view, differences detection can appear as irrelevant, as one is clearly able to see that something changed on the selected example without any help. But difference models can be computed at any scale. For example, taking the example of the Amriswil city : View of Amriswil model in 2020-10-13 (left) and 2020-10-17 (right) - Data : Kanton Thurgau It becomes more complicated to track down the differences that can appear between the two temporal versions. By computing their difference model , one is able to access a third model that ease the analysis of the evolution at the scale of the city itself as illustrated on the following image : Difference model computed for the city of Amriswil - Data : Kanton Thrugau One can see how difference models can be used to track down modifications brought to the land register in a simple manner, while keeping the information of the unchanged elements between the two compared models. This demonstrates that information that exists between models can be extracted and represented for further users or automated processes. In addition, such difference models can be computed at any scale, considering small area up to the whole countries. Difference Models : An Assessment Tool \u00b6 On the previous section, the difference models are computed using two models only separated of a few days, containing only a small amount of clear and simple modifications. This section focuses on detecting differences on larger models, separated by several years. In this case, the land register of the Canton of Geneva is considered : Illustration of the Geneva land register in 2017-04 (left) and 2019-04 (right) - Data : Canton of Geneva One can see that at such a scale, taking into account that the Canton of Geneva is one of the smallest in Switzerland, having a vision and a clear understanding of the modifications made between these two models is difficult by considering the two models separately. It's precisely where differences models can be useful to understand and analyze the evolution of the land register, along both the space and time dimensions. Large-Scale Analysis \u00b6 A first large-scale evaluation can be made on the overall models. A difference model can be computed considering the land register of Geneva in 2019 and 2017 as illustrated on the following image : Difference model on Geneva land register between 2019-04 and 2017-04 - Data : Canton of Geneva Two observations can be already made by looking at the difference model . In the first place, one can see that the amount of modifications brought to the land register is large in only two years. A large portion of the land register were subject to modifications or corrections, the unchanged parts being mostly limited outside the populated area. In the second place, one can observe large portions where differences seem to be accumulating over this period of time. Looking at them more closely leads to the conclusion that these zones were actually completely modified, as all elements are highlighted by the difference detection algorithm. The following image gives a closer view of such an area of differences accumulation : Focus on Carouge area of the 2019-04 and 2017-04 difference model - Data : Canton of Geneva Despite the amount of modifications outside this specific zone is also high, it is clear that the pointed zone contains more of them. Looking at it more closely leads to the conclusion that everything changed. In order to understand these areas of differences accumulation, the the land register experts of the Canton of Geneva ( SITG ) were questioned. They provided an explanation for these specific areas. Between 2017 and 2019 , these areas were subjected to a global correction in order to release the tension between the old reference frame LV03 [3] and the current one LV95 [4]. These corrections were made using the FINELTRA algorithm to modify the elements of the land register of the order of a few centimeters. The land register of Geneva provided the following illustration summarizing these reference frame corrections made between 2017 and 2019 on the Geneva territory : Reference frame corrections performed between 2017 and 2019 - Data : SITG Comparing this map from the land register with the computed model allows seeing how differences detection can emphasize this type of corrections efficiently, as the corrected zones on this previous image corresponds to the difference accumulation areas on the computed difference model . Small-Scale Analysis \u00b6 One can also dive deep into the details of the difference models . As we saw on the large scale analysis, two types of areas can be seen on the 2019-04-2017-04 difference model of Geneva : regular evolution with an accumulation of corrections and areas on which global corrections were applied. The following images propose a close view of these two types of situation : Illustration of the two observed type of evolutions of the land register - Data : Canton of Geneva On the left image above, one can observe the regular evolution of the land register where modifications are brought to the model in order to follow the evolution of the territory. On the right image above, one can see a close view of an area subjected to a global correction (reference frame), leading to a difference model highlighting all the elements. Analyzing more closely the right image above lead the observer to conclude that not all the elements are actually highlighted by the difference detection algorithm. Indeed, some elements are rendered in gray on the difference model , indicating their lack of modification between the two compared times. The following image emphasizes the unchanged elements that can be observed : Unchanged elements in the land register after reference frame correction - Data : SITG These unchanged elements can be surprising as they're found in an area that was subject to a global reference frame correction. This shows how difference models can be helpful to track down these type of events in order to check whether these unchanged elements are expected or are the results of a discrepancy in the land register evolution. Other example can be found in this very same area of the Geneva city. The following images give an illustration of two other close view where the unchanged element can be seen despite the reference frame correction : Unchanged elements in the land register after reference frame correction - Data : SITG On the left image above, one can observe that the unchanged elements are the railway tracks within the commune of Carouge . This is an interesting observation, as railway tracks can be considered as specific elements that can be subjected to different legislations regarding the land register. But it is clear that railway tracks were not considered in the reference frame correction. On the right image above, one can see another example of unchanged elements that are more complicated to explain, as they're in the middle of modified other elements. This clearly demonstrate how difference models can be helpful for analyzing and assessing the evolution of the land register models. Such models are able to drive users or automated processes and lead them to focus on relevant aspects and to define the good question in the context of analyzing the evolution of the land register. Conclusion \u00b6 The presented difference models computed based on two temporal versions of the land register and using the 4D framework algorithm showed how differences can be emphasized for users and automated processes [1]. Difference models can be helpful to determine the amount and nature of changes that appear in the land register. Applying such an algorithm on land register is especially relevant as it is a highly living model, that evolves jointly with the territory it describes. Two main applications can be considered using difference models applied on the land register. In the first place, the difference models can be used to assess and analyze the regular evolution of the territory. Indeed, updating the land register is not a simple task. Such modifications involve a whole chain of decisions and verifications, from surveyors to the highest land register authority before to be integrated in the model. Being able to assess and analyze the modifications in the land register through difference models could be one interesting strengthening of the overall process. The second application of difference models could be as an assessment tool of global corrections applied to the land register or parts of it. These modifications are often linked to the reference frame and its evolution. Being able to assess the corrections through the difference models could add a helpful tool in order to verify that the elements of the land register where correctly processed. In this direction, difference models could be used during the introduction of the DM.flex reference frame for both analyzing its introduction and demonstrating that difference models can be an interesting point of view. Reproduction Resources \u00b6 To reproduce the presented experiments, the STDL 4D framework has to be used and can be found here : STDL 4D framework (eratosthene-suite), STDL You can follow the instructions on the README to both compile and use the framework. Unfortunately, the used data are not currently public. In both cases, the land register INTERLIS datasets were provided to the STDL directly. You can contact both Thurgau Kanton and SITG : INTERLIS land register, Thurgau Kanton INTERLIS land register, SITG (Geneva) to query the data. In order to extract and convert the data from the INTERLIS models, the following code is used : INTERLIS to UV3 (dalai-suite), STDL/EPFL where the README gives all the information needed. For the 3D geographical coordinates conversion and heights restoration, we used two STDL internal tools. You can contact the STDL to obtain the tools and support in this direction : ptolemee-suite : 3D coordinate conversion tool (EPSG:2056 to WGS84) height-from-geotiff : Restoring geographical heights using topographic GeoTIFF ( SRTM ) You can contact STDL for any question regarding the reproduction of the presented results. References \u00b6 [1] Automatic Detection of Changes in the Environment, N. Hamel, STDL 2020 [2] DM.flex reference frame [3] LV03 Reference frame [4] LV95 Reference frame","title":"DIFFERENCE MODELS APPLIED ON LAND REGISTER"},{"location":"TASK-DTRK/#difference-models-applied-on-land-register","text":"Nils Hamel (UNIGE) - Huriel Reichel (swisstopo) Project scheduled in the STDL research roadmap - TASK-DTRK September 2020 to November 2020 - Published on ? ?, 2021 Abstract : Being able to track modifications in the evolution of geographical datasets is one important aspect in territory management, as a large amount of information can be extracted out of differences models. Differences detection can also be a tool used to assess the evolution of a geographical model through time. In this research project, we apply differences detection on INTERLIS models of the official Swiss land registers in order to emphasize and follow its evolution and to demonstrate that change in reference frames can be detected and assessed.","title":"DIFFERENCE MODELS APPLIED ON LAND REGISTER"},{"location":"TASK-DTRK/#introduction","text":"Land register models are probably to most living of the geographical models as they are constantly updated to offer a rigorous and up-to-date view of the territory. The applied corrections are always the result of a complex process, involving different territory actors, until the decision is made to integrate them into the land register. In addition, land register models comes with an additional constraint linked to political decisions. Indeed, the land register models are the result of a political mission conducted under federal laws making these models of high importance and requiring constant care. We show in this research project how differences detection tool [1] of the STDL 4D framework can be used to emphasize and analyze these corrections along the time dimension. In addition to the constant updates of the models, changes in the reference frame can also lead to large-scale corrections of the land register models. These global corrections are then made even more complex by the federal laws that impose a high degree of correctness and accuracy. In the context of the introduction of the new reference frame DM.flex [2] for the Swiss land register, being able to assess the applied changes on the geographical model appear as an important aspect. Indeed, changing the reference frame for the land register models is a long and complex technical process that can be error prompt. We also show in this research project how the difference detection algorithm can be helpful to assess and verify the performed corrections.","title":"Introduction"},{"location":"TASK-DTRK/#research-project-specifications","text":"In this research project, the difference detection algorithm implemented in the STDL 4D framework is applied on INTERLIS data containing the official land register models of different Swiss Canton . As introduced, two main directions are considered for the difference detection algorithm : Demonstrating the ability to extract information in between land register models Demonstrating the ability of difference models to be used as an assessment tool Through the first direction, the difference detection algorithm is presented. Considering the difference models it allows computing, it is shown how such model are able to extract information in between the models in order to emphasize the ability to represent, and then, to verify the evolution of the land register models. The second direction focuses on demonstrating that difference models are a helpful representation of the large-scale corrections that can be applied to land register during reference frame modification and how they can be used as a tool to assess the modifications and to help to fulfil the complex task of the verification of the corrected models.","title":"Research Project Specifications"},{"location":"TASK-DTRK/#research-project-data","text":"For the first research direction, the land register models of the Thurgau Kanton are considered. They are selected in order to have a small temporal distance allowing to focus on a small amount of well-defined differences : Thurgau Kanton , 2020-10-13 , INTERLIS Thurgau Kanton , 2020-10-17 , INTERLIS For the second direction, which focus on more complex differences, the models of the Canton of Geneva land register are considered with a much larger temporal gap between them : Canton of Geneva , 2009-10 , INTERLIS Canton of Geneva , 2013-04 , INTERLIS Canton of Geneva , 2017-04 , INTERLIS Canton of Geneva , 2019-04 , INTERLIS","title":"Research Project Data"},{"location":"TASK-DTRK/#difference-models-a-temporal-derivative","text":"This first section focuses on short-term differences to show how difference models work and how they are able to represent the modifications extracted out of the two compared models. The following images give an illustration of the considered dataset, which are the land register models of Thurgau Kanton : Illustration of Thurgau Kanton INTERLIS models - Data : Kanton Thurgau The models are made of vector lines, well geo-referenced in the Swiss coordinates frame EPSG:2056 . The models are also made of different layers that are colored differently with the following correspondences : INTERLIS selected topics and tables colors - Official French and German designations These legends are used all along this research project. Considering two temporal versions of this geographical model, separated of a few days, one is able to extract difference models using the 4D framework algorithm. As an example, one can consider this very specific view of the land register, focusing on a few houses : Close view of the Thurgau INTERLIS model in 2020-10-13 (left) and 2020-10-17 (right) - Data : Kanton Thurgau It is clear that most of the close view is identical for the two models, except for a couple of houses that were added to the land register model between these two temporal versions. By applying the difference detection algorithm, one is able to obtain a difference model comparing the two previous models. The following image gives an illustration of the obtained difference models considering the most recent temporal version as reference : Difference model obtained comparing the two temporal versions - Data : Kanton Thrugau One can see how the difference algorithm is able to emphasize the differences and to represent them in a human-readable third model. The algorithm also displays the identical parts in dark gray to offer the context of the differences to the operator. Of course, in such close view, differences detection can appear as irrelevant, as one is clearly able to see that something changed on the selected example without any help. But difference models can be computed at any scale. For example, taking the example of the Amriswil city : View of Amriswil model in 2020-10-13 (left) and 2020-10-17 (right) - Data : Kanton Thurgau It becomes more complicated to track down the differences that can appear between the two temporal versions. By computing their difference model , one is able to access a third model that ease the analysis of the evolution at the scale of the city itself as illustrated on the following image : Difference model computed for the city of Amriswil - Data : Kanton Thrugau One can see how difference models can be used to track down modifications brought to the land register in a simple manner, while keeping the information of the unchanged elements between the two compared models. This demonstrates that information that exists between models can be extracted and represented for further users or automated processes. In addition, such difference models can be computed at any scale, considering small area up to the whole countries.","title":"Difference Models : A Temporal Derivative"},{"location":"TASK-DTRK/#difference-models-an-assessment-tool","text":"On the previous section, the difference models are computed using two models only separated of a few days, containing only a small amount of clear and simple modifications. This section focuses on detecting differences on larger models, separated by several years. In this case, the land register of the Canton of Geneva is considered : Illustration of the Geneva land register in 2017-04 (left) and 2019-04 (right) - Data : Canton of Geneva One can see that at such a scale, taking into account that the Canton of Geneva is one of the smallest in Switzerland, having a vision and a clear understanding of the modifications made between these two models is difficult by considering the two models separately. It's precisely where differences models can be useful to understand and analyze the evolution of the land register, along both the space and time dimensions.","title":"Difference Models : An Assessment Tool"},{"location":"TASK-DTRK/#large-scale-analysis","text":"A first large-scale evaluation can be made on the overall models. A difference model can be computed considering the land register of Geneva in 2019 and 2017 as illustrated on the following image : Difference model on Geneva land register between 2019-04 and 2017-04 - Data : Canton of Geneva Two observations can be already made by looking at the difference model . In the first place, one can see that the amount of modifications brought to the land register is large in only two years. A large portion of the land register were subject to modifications or corrections, the unchanged parts being mostly limited outside the populated area. In the second place, one can observe large portions where differences seem to be accumulating over this period of time. Looking at them more closely leads to the conclusion that these zones were actually completely modified, as all elements are highlighted by the difference detection algorithm. The following image gives a closer view of such an area of differences accumulation : Focus on Carouge area of the 2019-04 and 2017-04 difference model - Data : Canton of Geneva Despite the amount of modifications outside this specific zone is also high, it is clear that the pointed zone contains more of them. Looking at it more closely leads to the conclusion that everything changed. In order to understand these areas of differences accumulation, the the land register experts of the Canton of Geneva ( SITG ) were questioned. They provided an explanation for these specific areas. Between 2017 and 2019 , these areas were subjected to a global correction in order to release the tension between the old reference frame LV03 [3] and the current one LV95 [4]. These corrections were made using the FINELTRA algorithm to modify the elements of the land register of the order of a few centimeters. The land register of Geneva provided the following illustration summarizing these reference frame corrections made between 2017 and 2019 on the Geneva territory : Reference frame corrections performed between 2017 and 2019 - Data : SITG Comparing this map from the land register with the computed model allows seeing how differences detection can emphasize this type of corrections efficiently, as the corrected zones on this previous image corresponds to the difference accumulation areas on the computed difference model .","title":"Large-Scale Analysis"},{"location":"TASK-DTRK/#small-scale-analysis","text":"One can also dive deep into the details of the difference models . As we saw on the large scale analysis, two types of areas can be seen on the 2019-04-2017-04 difference model of Geneva : regular evolution with an accumulation of corrections and areas on which global corrections were applied. The following images propose a close view of these two types of situation : Illustration of the two observed type of evolutions of the land register - Data : Canton of Geneva On the left image above, one can observe the regular evolution of the land register where modifications are brought to the model in order to follow the evolution of the territory. On the right image above, one can see a close view of an area subjected to a global correction (reference frame), leading to a difference model highlighting all the elements. Analyzing more closely the right image above lead the observer to conclude that not all the elements are actually highlighted by the difference detection algorithm. Indeed, some elements are rendered in gray on the difference model , indicating their lack of modification between the two compared times. The following image emphasizes the unchanged elements that can be observed : Unchanged elements in the land register after reference frame correction - Data : SITG These unchanged elements can be surprising as they're found in an area that was subject to a global reference frame correction. This shows how difference models can be helpful to track down these type of events in order to check whether these unchanged elements are expected or are the results of a discrepancy in the land register evolution. Other example can be found in this very same area of the Geneva city. The following images give an illustration of two other close view where the unchanged element can be seen despite the reference frame correction : Unchanged elements in the land register after reference frame correction - Data : SITG On the left image above, one can observe that the unchanged elements are the railway tracks within the commune of Carouge . This is an interesting observation, as railway tracks can be considered as specific elements that can be subjected to different legislations regarding the land register. But it is clear that railway tracks were not considered in the reference frame correction. On the right image above, one can see another example of unchanged elements that are more complicated to explain, as they're in the middle of modified other elements. This clearly demonstrate how difference models can be helpful for analyzing and assessing the evolution of the land register models. Such models are able to drive users or automated processes and lead them to focus on relevant aspects and to define the good question in the context of analyzing the evolution of the land register.","title":"Small-Scale Analysis"},{"location":"TASK-DTRK/#conclusion","text":"The presented difference models computed based on two temporal versions of the land register and using the 4D framework algorithm showed how differences can be emphasized for users and automated processes [1]. Difference models can be helpful to determine the amount and nature of changes that appear in the land register. Applying such an algorithm on land register is especially relevant as it is a highly living model, that evolves jointly with the territory it describes. Two main applications can be considered using difference models applied on the land register. In the first place, the difference models can be used to assess and analyze the regular evolution of the territory. Indeed, updating the land register is not a simple task. Such modifications involve a whole chain of decisions and verifications, from surveyors to the highest land register authority before to be integrated in the model. Being able to assess and analyze the modifications in the land register through difference models could be one interesting strengthening of the overall process. The second application of difference models could be as an assessment tool of global corrections applied to the land register or parts of it. These modifications are often linked to the reference frame and its evolution. Being able to assess the corrections through the difference models could add a helpful tool in order to verify that the elements of the land register where correctly processed. In this direction, difference models could be used during the introduction of the DM.flex reference frame for both analyzing its introduction and demonstrating that difference models can be an interesting point of view.","title":"Conclusion"},{"location":"TASK-DTRK/#reproduction-resources","text":"To reproduce the presented experiments, the STDL 4D framework has to be used and can be found here : STDL 4D framework (eratosthene-suite), STDL You can follow the instructions on the README to both compile and use the framework. Unfortunately, the used data are not currently public. In both cases, the land register INTERLIS datasets were provided to the STDL directly. You can contact both Thurgau Kanton and SITG : INTERLIS land register, Thurgau Kanton INTERLIS land register, SITG (Geneva) to query the data. In order to extract and convert the data from the INTERLIS models, the following code is used : INTERLIS to UV3 (dalai-suite), STDL/EPFL where the README gives all the information needed. For the 3D geographical coordinates conversion and heights restoration, we used two STDL internal tools. You can contact the STDL to obtain the tools and support in this direction : ptolemee-suite : 3D coordinate conversion tool (EPSG:2056 to WGS84) height-from-geotiff : Restoring geographical heights using topographic GeoTIFF ( SRTM ) You can contact STDL for any question regarding the reproduction of the presented results.","title":"Reproduction Resources"},{"location":"TASK-DTRK/#references","text":"[1] Automatic Detection of Changes in the Environment, N. Hamel, STDL 2020 [2] DM.flex reference frame [3] LV03 Reference frame [4] LV95 Reference frame","title":"References"},{"location":"TASK-IDET/","text":"STDL Object Detection Framework \u00b6 Adrian F. Meyer, FHNW Alessandro Cerioni, Etat de Geneve Current Mandate Schedule : October 2020 to September 2021 (with extension options until 2024) Objective \u00b6 This strategic component of the STDL consists of the automated analysis of geospatial images using deep learning while providing practical applications for specific use cases. The overall goal is the extraction of semantic information from remote sensing data. The earlier involved case studies revolve around concrete object detection use cases deploying modern machine learning methods and utilizing a multitude of available datasets. Later, full semantic surface layers can be produced leveraging the obtained datasets to arrive at a prototypical platform for object detection which is highly useful for decision makers at various contact points in society. Generating a Model from Cadastral Vectors and Aerial Images to Predict Objects in the Same or a New Area of Interest (AoI). Background and Potential Use Cases \u00b6 Swimming Pools \u00b6 Providing a reliable detection of swimming pools allows authorities to assess the status quo to update archival datasets and to reinforce administrative construction permit processes. The status quo is based on manually digitized cadastral information. This data is used to extract feature masks which can be applied to orthophoto imagery such as the SWISSIMAGE dataset or aerial photos provided by the end users. Deep Learning algorithms such as Faster RCNN or Mask RCNN then allow the detection of previously unregistered swimming pools in a defined perimeter. Achievable detection accuracies range above 90% (F1 Score). Current users of the technology include the Canton of Geneva and the Canton of Neuch\u00e2tel. Example Detections of Swimming Pools Missing in the Cadastre. Thermal Panels \u00b6 Thermal Solar Panels Can Contribute a Substantial Amount to Reducing Carbon Emissions. Silage Hay Bales \u00b6 Locations of Silage Hay Bales are Relevant for the Calculation of Governmental Agriculture Subsidies. Methodology \u00b6 Area of Interest Preparation \u00b6 Slippy Map Tiles Tiling Subsystem \u00b6 WMS and MIL Support Slippy Map Tiles Detectron2 Core \u00b6 Dataset Splitting Subdivision of Ground Truth Datasets Transfer Learning Subdivision of Ground Truth Datasets Hyperparameter Tuning Parallelisation \u00b6 High-Performance Computing Cluster at FHNW HPE Apollo 6500 with 4 NVidia V100 GPUs Joblib Backend Datasets \u00b6 Ground Truth Labels \u00b6 Swimming Pools of Geneva and Neuch\u00e2tel (partial) Label inputs for deep learning derived from cadastral data Approximately 3000 cross checked swimming pool annotations are available as vectorized shapes in the Cantons of Geneva and partially in Neuch\u00e2tel. They are compatible with orthophotos from 2018/19 such as the latest SWISSIMAGE 10cm layer. Thermal Panels in Northwestern Switzerland The thermal panel dataset of the STDL is based on a predecessor project. The project \u00abSolAi\u00bb was launched in 2018 at the Institute of Geomatics (IGEO) of the University of Applied Sciences Northwestern Switzerland (FHNW), in collaboration with the Swiss Federal Office of Energy (SFOE) and will be finished by the end of 2020. The project aims to use Mask R-CNN algorithms to automatically identify and quantify existing solar installations from Swissimage orthophotos. Such an approach should then serve as a basis for the implementation of the energy strategy and statistical estimation models of the solar market. A solar register is already available in Switzerland based on applications of government subsidies. This dataset is lacking integral completeness as well as absolute shape and positions of the installations though. To date, Switzerland still lacks reliable position and area data of the photovoltaic and solar thermal systems already installed in order to enable a complete evaluation in conjunction with the solar cadastre data. In the scope of the project over 30'000 polygons of solar panels classified into \"Photovoltaic\" and \"Thermal\" installations were drawn over the SWISSIMAGE dataset to generate traning data, currently yielding a mean average prediction accuracy of ~87%. The Predecessor-Project \u00abSolAi\u00bb was funded by the Swiss Federal Office of Energy Various cantonal and federal authorities as well as research groups have shown exceptional interest in obtaining the dataset and building on these scientific findings. The use case will therefore be continued in the framework of the Swiss Territorial Data Lab project to refine the scope of the outcomes to end user needs and achieve a maturation of the classification results through hyperparameter optimization, retraining on multispectral imagery and evaluating prediction/inferencing robustness in different radiometric scenarios. Early Detection Result of a Resnet-50 Mask-RCNN architecture Silage Hay Bales in Thurgau 700 Labels as Vectors. Data Sources \u00b6 SWISSIMAGE RGB 10cm by swisstopo SWISSIMAGE RS 10cm by swisstopo Labels drawn from the official Swiss cadastral services. The Swiss cadastral system comprises the cadastral surveying, the Cadastre of Public-law Restrictions on landownership (PLR-cadastre) and the land register. SITG - Geneva Geodata Services SITN - Neuch\u00e2tel Geodata Services Results \u00b6 Metrics Output \u00b6 Example Detections of Swimming Pools Missing in the Cadastre. Swimming Pools \u00b6 Zoom Level Dependency Detected Swimming Pools in the Canton of Neuch\u00e2tel Silage Hay Bales \u00b6 Early Detection Result of a Resnet-50 Mask-RCNN architecture Outlook \u00b6 Ultimately, an automatized system for surface classification based on aerial imagery and additional data sources will be proposed that allows consistent differential surface segmentation as a basis for differential change analysis. Spatiotemporal data storage provides insight into current, historical and future territorial features scalabe from small communal objects to the environmental and landscape levels. Links and Resources \u00b6 Institute Geomatics FHNW Project Area Statistics Project Sol Ai Project Animal Detection Swisstopo BFS / OFS BFE / Energiestrategie 2050","title":"**STDL Object Detection Framework**"},{"location":"TASK-IDET/#stdl-object-detection-framework","text":"Adrian F. Meyer, FHNW Alessandro Cerioni, Etat de Geneve Current Mandate Schedule : October 2020 to September 2021 (with extension options until 2024)","title":"STDL Object Detection Framework"},{"location":"TASK-IDET/#objective","text":"This strategic component of the STDL consists of the automated analysis of geospatial images using deep learning while providing practical applications for specific use cases. The overall goal is the extraction of semantic information from remote sensing data. The earlier involved case studies revolve around concrete object detection use cases deploying modern machine learning methods and utilizing a multitude of available datasets. Later, full semantic surface layers can be produced leveraging the obtained datasets to arrive at a prototypical platform for object detection which is highly useful for decision makers at various contact points in society. Generating a Model from Cadastral Vectors and Aerial Images to Predict Objects in the Same or a New Area of Interest (AoI).","title":"Objective"},{"location":"TASK-IDET/#background-and-potential-use-cases","text":"","title":"Background and Potential Use Cases"},{"location":"TASK-IDET/#swimming-pools","text":"Providing a reliable detection of swimming pools allows authorities to assess the status quo to update archival datasets and to reinforce administrative construction permit processes. The status quo is based on manually digitized cadastral information. This data is used to extract feature masks which can be applied to orthophoto imagery such as the SWISSIMAGE dataset or aerial photos provided by the end users. Deep Learning algorithms such as Faster RCNN or Mask RCNN then allow the detection of previously unregistered swimming pools in a defined perimeter. Achievable detection accuracies range above 90% (F1 Score). Current users of the technology include the Canton of Geneva and the Canton of Neuch\u00e2tel. Example Detections of Swimming Pools Missing in the Cadastre.","title":"Swimming Pools"},{"location":"TASK-IDET/#thermal-panels","text":"Thermal Solar Panels Can Contribute a Substantial Amount to Reducing Carbon Emissions.","title":"Thermal Panels"},{"location":"TASK-IDET/#silage-hay-bales","text":"Locations of Silage Hay Bales are Relevant for the Calculation of Governmental Agriculture Subsidies.","title":"Silage Hay Bales"},{"location":"TASK-IDET/#methodology","text":"","title":"Methodology"},{"location":"TASK-IDET/#area-of-interest-preparation","text":"Slippy Map Tiles","title":"Area of Interest Preparation"},{"location":"TASK-IDET/#tiling-subsystem","text":"WMS and MIL Support Slippy Map Tiles","title":"Tiling Subsystem"},{"location":"TASK-IDET/#detectron2-core","text":"","title":"Detectron2 Core"},{"location":"TASK-IDET/#parallelisation","text":"","title":"Parallelisation"},{"location":"TASK-IDET/#datasets","text":"","title":"Datasets"},{"location":"TASK-IDET/#ground-truth-labels","text":"","title":"Ground Truth Labels"},{"location":"TASK-IDET/#data-sources","text":"SWISSIMAGE RGB 10cm by swisstopo SWISSIMAGE RS 10cm by swisstopo Labels drawn from the official Swiss cadastral services. The Swiss cadastral system comprises the cadastral surveying, the Cadastre of Public-law Restrictions on landownership (PLR-cadastre) and the land register. SITG - Geneva Geodata Services SITN - Neuch\u00e2tel Geodata Services","title":"Data Sources"},{"location":"TASK-IDET/#results","text":"","title":"Results"},{"location":"TASK-IDET/#metrics-output","text":"Example Detections of Swimming Pools Missing in the Cadastre.","title":"Metrics Output"},{"location":"TASK-IDET/#swimming-pools_1","text":"Zoom Level Dependency Detected Swimming Pools in the Canton of Neuch\u00e2tel","title":"Swimming Pools"},{"location":"TASK-IDET/#silage-hay-bales_1","text":"Early Detection Result of a Resnet-50 Mask-RCNN architecture","title":"Silage Hay Bales"},{"location":"TASK-IDET/#outlook","text":"Ultimately, an automatized system for surface classification based on aerial imagery and additional data sources will be proposed that allows consistent differential surface segmentation as a basis for differential change analysis. Spatiotemporal data storage provides insight into current, historical and future territorial features scalabe from small communal objects to the environmental and landscape levels.","title":"Outlook"},{"location":"TASK-IDET/#links-and-resources","text":"Institute Geomatics FHNW Project Area Statistics Project Sol Ai Project Animal Detection Swisstopo BFS / OFS BFE / Energiestrategie 2050","title":"Links and Resources"},{"location":"TASK-REGBL/","text":"COMPLETION OF THE FEDERAL REGISTER OF BUILDINGS AND DWELLINGS \u00b6 Nils Hamel (UNIGE) - Huriel Reichel (swisstopo) Proposed by the Federal Statistical Office - TASK-REGBL December 2020 to February 2021 - Published on March 2, 2021 Abstract : The Swiss Federal Statistical Office is in charge of the national Register of Buildings and Dwellings (RBD) which keeps track of every existing building in Switzerland. Currently, the register is being completed with buildings in addition to regular dwellings to offer a reliable and official source of information. The completion of the register introduced issues due to missing information and their difficulty to be collected. The construction year of the buildings is one missing information for a large amount of register entries. The Statistical Office mandated the STDL to investigate on the possibility to use the Swiss National Maps to extract this missing information using an automated process. A research was conducted in this direction with the development of a proof-of-concept and a reliable methodology to assess the obtained results. Introduction \u00b6 The Swiss Federal Statistical Office [1] is responsible of maintaining the Federal Register of Buildings and Dwellings ( RBD ) in which a collection of information about buildings and homes are stored. Currently, a completion operation of the register is being conducted to include to it any type of construction on the Swiss territory. Such completion operation comes with many challenges including the gathering of the information related to the construction being currently integrated to the register. In this set of information are the construction years of the buildings. Such information is important to efficiently characterise each Swiss building and to allow the Statistical Office to provide a reliable register to all actors relying on it. The construction year of buildings turns out to be complicated to gather, as adding new buildings to the register already impose an important workload even for the simple information. In addition, in many cases, the construction year of the building is missing or can not be easily collected to update the register. The Statistical Office mandated the STDL to perform researches on the possibility to automatically gather the construction year by analysing the swisstopo [3] National Maps [4]. Indeed, the Swiss national maps are known for their excellency, their availability on any geographical area, and for their temporal cover. The national maps are made with a rigorous and well controlled methodology from the 1950s and therefore they can be used as a reliable source of information to determine the buildings' construction year. The STDL was then responsible for performing the researches and developing a proof-of-concept to provide all the information needed to the Statistical Office for them to take the right decision on considering national maps as a reliable way of assigning a construction year for the buildings lacking information. Research Project Specifications \u00b6 Extracting the construction date out of the national maps is a real challenge, as the national maps are a heavy dataset, they are not easy to be considered as a whole. In addition, the Statistical Office needs the demonstration that it can be done in a reliable way and within a reasonable amount of time to limit the cost of such process. They are also subjected to strict tolerances on the efficiency of the construction years extraction through an automated process. The goal of at least 80 % of overall success was then provided as a constraint to the STDL . As a result, the research specifications for the STDL were: Gathering and understanding the data related to the problem Developing a proof-of-concept demonstrating the possibility to extract the construction years from the national maps Assessing the results with a reliable metric to allow demonstrating the quality and reliability of the obtained construction years Research Data & Selected Areas \u00b6 In this research project, two datasets were considered: the building register itself and the national maps. As both datasets are heavy and complex, considering them entirely for such a research project would have been too complicated and unnecessary. It was then decided to focus on four areas selected for their representativeness of Swiss landscape: Basel (BS): Urban area Bern (BE): Urban and peri-urban area Biasca (TI): Rural and mountainous Caslano (TI): Peri-urban and rural The following images give a geographical illustration of the selected areas through their most recent map: Illustration of the selected areas: Basel (2015), Bern (2010), Biasca (2012) and Caslano (2009) Data: swisstopo Basel was selected as it was one example of an area on which the building register was already well filled in terms of construction years. The four regions are 6km by 6km squared areas which allows up to twenty thousand buildings to be considered on a single one. Federal Register of Buildings and Dwellings \u00b6 The register of buildings is a formal database composed with entries, each of them representing a specific building. Each entry comes with a set of information related to the building they describe. In this project, a sub-set of these informations was considered: Federal identifier of the building (EGID) The position of the building, expressed in the EPGS:2056 (GKODE, GKODN) The building construction year, when available (GBAUJ) The surface of the building, when available, expressed in square metres (GAREA) In addition, tests were conducted by considering the position of the entries of each building. In turned out rapidly that they were not useful in this research project as they were missing on a large fraction on the register and only providing a redundant information according to the position of the buildings. The following table gives a summary of the availability of the construction year in the register according to the selected areas: Area Buildings Available years Missing fraction Basel 17\u2019088 16\u2019584 3% Bern 21\u2019251 4\u2019499 79% Biasca 3\u2019774 1\u2019346 64% Caslano 5\u2019252 2\u2019452 53% One can see that the amount of missing construction year can be large depending on the considered area. National Maps \u00b6 On the side of the national maps, the dataset is more complex. In addition to the large number of available maps, variations of them can also be considered. Indeed, maps are made for different purposes and come with variations in their symbology to emphasise elements on which they focus. Moreover, for modern years, sets of vector data can also be considered in parallel to maps. Vector data are interesting as they allow to directly access the desired information, that is the footprint of the building without any processing required. The drawback of the vector data is their temporal coverage which is limited to the last ten to twenty years. The following images give an illustration of the aspect of the available maps and vector datasets considering the example of the Bern area. Starting with the traditional maps : Available map variations: KOMB, KGRS and KREL - Data: swisstopo and the more specific and vector ones: Available map variations: SITU, GEB and DKM25-GEB (vector) - Data: swisstopo In addition to the number of available variations and data types, they all come with their specific temporal coverage. In the case of this research project, we tried to go back in time as much as possible, simplifying the choice for the older maps. The question still remains for more modern times. As we are mostly interesting in buildings, the availability of already extracted building layers, that can be either raster or vector data, is highly interesting. But the problem of data selection is complex in our case. Indeed, no matter the choice, on the older times, the only available maps have to be considered. In addition to building footprint access, the question of the continuity of the data as to be considered with care. More than building footprints, we are interested in the continuity of these footprints, in order to be able to safely assume the cycle of life of the tracker buildings. This consideration led us to discover variation in methodologies depending on the considered set of data. Indeed, buildings are not shaped in the same way on traditional maps than they are in layer focusing on them. It follows that variation of the symbology, so do the shape of the building, appears between traditional maps and building layers (raster and vector). These variations can lead to shocks going from a map to the one preceding it in time. This can break the continuity of the building footprints along time, making them much more difficult to track safely. This is the reason we choose to focus on the KOMB variation of the maps. These maps are very stable and covers the largest temporal ranges. The methodology was kept very similar along the years, making this dataset much more reliable to work with when the time dimension is considered. Only considering the KOMB variation of the maps also allows to ensure that all source data is treated the same in the processing pipeline, easing the assessment of the results. In addition, the KOMB maps are dense in information and come with colorimetry of their symbology. This opens the possibility to more easily extract the information we need in this project, that are the building footprints. One exception was made concerning the KOMB maps: in their very latest version, the methodology changed, causing the symbology to be different with the older KOMB maps. In their latest version, texts are much more numerous and tend to cover a large amount of the buildings, making them invisible . For this reason, their latest version was dropped, slightly reducing the temporal coverage on the 2015-2020 period. Selecting the KOMB variation allowed us to obtain the following temporal coverage for the four selected areas: Area Oldest map Latest map Mean separation Basel 1955 2015 5.5 Years Bern 1954 2010 5.6 Years Biasca 1970 2012 6.0 Years Caslano 1953 2009 6.2 Years One can see that a large portion of the 20th century can be covered using the maps with a very good resolution of around five to six years between the maps. Research Approaches \u00b6 In this research project, the main focus was put on the national maps to extract the construction year of buildings as the maps are sources on which we can rely and assess the results. The only drawback of the maps is their limited temporal coverage, as they only start to be available in the 1950s . This is the reason why another experimental approach was also added to address the cases of building being built before the 1950s . This secondary approach focused on a statistical methodology to verify to which extent it could be possible to assign a construction date even in the case no maps are available. National Maps : This main approach focuses on the national maps from which the construction year of a building is deduced from a temporal analysis of the maps. Each building is tracked until it disappears or change its shape on a given map allowing to deduce that the construction of the building was made in the gap separating the map and its successor one. Statistical Analysis : This method is based on the principle of spatial dependence and furthermore on concentric zones of urban development. This is technically an interpolator which deduces construction years based first on different searching radii for difference variances, second by splitting the data in quantiles and, finally, by a gaussian mixture model unsupervised learning technique to gather the final predictions. The statistical analysis allows then to consider buildings that were detected on all maps, meaning their construction is older than the oldest available map, to assign them an estimation of their construction year, knowing they had to be older than the oldest map. Research Approach: National Maps \u00b6 In order to detect construction year of buildings, we need to be able to track them down on the maps across the temporal coverage. The RBD is providing the reference list of the building, each coming with a federal identifier ( EGID ) and a position. This position can then be used to track down the building on maps for its appearance or morphological change. As the maps are already selected, as the research areas, this research approach can be summarised in the following way: Translating maps into binary images containing only building Extracting the RBD buildings related to the analysed area Detection procedure of the buildings on the maps Detection of the morphological variation of the buildings Assessment of the obtained results The four first points are related to the development of the proof-of-concept. The last one concern a very sensitive and complicated question relative to the considered problem: how to analyse and assess the obtained results. This question was to most difficult question in this research, and finding a clear and reliable answer is mandatory before to develop anything. For this reason, it is considered in the first place. Reliability of the Data \u00b6 Assessing the results is essentially having a strong reference allowing to compare both in order to obtain a reliable characterisation of the success rate in the deduction of the construction years. This question leads to the discovery that this problem is much more complex that and can appear in the first place. Indeed, we were warned by the Statistical Office that the RBD , considering the construction years it already gives, can be unreliable on some of its portions. This can be explained by the fact that collecting such information is a long and complicated administrative process. As an example, the following image gives an illustration of a building tracked on each of the available selected maps: Temporal track of a selected building On this illustration, one can see two things: the RBD announce a construction year in 1985 ; the maps are clearly indicating something different, locating its construction year between 1963 and 1969 . So both datasets are contradicting each other. In order to solve the contradiction, we manually searched for historical aerial images. The following images illustrate what was found: Aerial view of the building situation: 1963, 1967 and 1987 - Data: swisstopo One can clearly see that the maps seem to give the correct answer concerning the construction date of this specific building, the RBD being contradicted by two other sources. This illustrates the fact that the RBD can not be directly considered as a reliable reference to assess the results. The same question applies for the maps. Even if it is believed that they are highly reliable, one has to be careful with such consideration. Indeed, looking at the following example: Temporal track of a selected building In this case, the RBD gives 1986 as the construction date of the pointed building. The maps are giving a construction year between 1994 and 2000 . Again, the two datasets are contradicting each other. The same procedure was conducted to solve the contradiction: Aerial view of the building situation: 1970, 1986 and 1988 - Data: swisstopo Looking at the aerial images, it seems that the tracked building was there in 1988 . One can see that the map in 1994 continue to represent the four old buildings instead on the new one. It's only in 2000 that the maps are correctly representing the new building. This shows that despite maps are a reliable source of geo-information, they can also be subjected to delay in their symbology. The maps also come with the problem of the consistency of the building footprint symbology. Looking at the following example: Temporal track of a selected building one can see that the maps seem to indicate a strange evolution of the situation: a first building appears in 1987 and it is destroyed and replaced by a larger one in 1993 . Then, this new large building seems to have been destroyed right after its construction to be replaced by a new one in 1998 . Considering aerial images of the building situation: Aerial image view of the building situation: 1981, 1987 and 1993 - Data: swisstopo one can clearly see that a first building was constructed and completed by an extension between 1987 and 1993 . This shows an illustration where the symbology of the building footprints can be subjected to variation than can be de-synchronised regarding the true situation. Metric \u00b6 In such context, neither the RBD or the national maps can be formally considered as a reference. It follows that we are left without a solution to assess our results, and more problematically, without any metric able to guide the developments of the proof-of-concept in the right direction. To solve the situation, one hypothesis is made in this research project. Taking into account both the RBD and the national maps, one can observe that both are built using methodologies that are very different. On one hand, the RBD is built out of a complex administrative process, gathering the required information in a step by step process, going from communes to cantons , and finally to the Statistical Office . On the other hand, the national maps are built using regular aerial image campaigns conducted over the whole Switzerland. The process of establishing maps is quite old and can then be considered as well controlled and stable. Both datasets are then made with methodologies that can be considered as fully independent from each other. This led us to the formulation of our hypothesis: Hypothesis : As the RBD and national maps are the results of independent methodologies, an error in one dataset is very unlikely to compensate an error in the other. In other words, if the RBD and the national maps agree on the construction year of a building, this information can be considered as a reliable reference, as it would be very unlikely to have two errors leading to such agreement. One should remain careful with this hypothesis, despite it sounds reasonable. It would be very difficult to assess it as requiring to gather complex confirmation data that would have to be independent of the RBD , the national maps and the aerial images (as maps are based on them). This assumption is the only one made in this research project. Accepting this assumption leads us to the possibility to establish a formal reference that can be used as a metric to assess the results and to guide the development of the proof-of-concept. But such reference has to be made with care, as the problem remains complex. To illustrate this complexity, the following figure gives a set representation of our problem: Set representation of the RBD completion problem The two rectangles represent the set of buildings for a considered area. On the left, one can see the building set from the RBD point of view. The grey area shows the building without the information of their construction year. Its complementary set is split in two sub-sets that are the buildings having a construction year that is absolutely correct and absolutely incorrect (the limit between both is subject to a bit of interpretation, as the construction year is not a strong concept). If a reference can be extracted, it should be in the green sub-set. The problem is that we have no way of knowing which building are in which sub-set. So the national maps were considered to define another sub-set: the synchronous sub-set where both RBD and national maps agree. To build the metric, the RBD sub-set of buildings coming with the information of the construction year is randomly sub-sampled to extract a representative sub-set: the potentials . This sub-set of potentials is then manually analysed to separate the building on which both datasets agree and to reject the other. At the end of the process, the metric sub-set is obtained and should remain representative. On the right of the set representation is the view of the buildings set through the national maps. One can see that the same sub-set appears but it replaces the construction years by the representation of the building on the maps. The grey part is then representing the building that are not represented on the maps because of their size or because they can be hidden by the symbology for example. The difference is that the maps do not give access to the construction years directly, but they are read from the maps through our developed detector. The detector having a success rate, it cuts the whole set of sub-sets in half, which is exactly what we need for out metric. If the metric sub-set remains representative, the success rate of the detector evaluated on it should generalise to the whole represented buildings. This set representation demonstrates that the problem is very complex and has to be handled with care. Considering only the six most important sub-set and considering construction year are extracted by the detector from the maps, it means that up to 72 specific case can apply on each building randomly selected. To perform the manual selection, a random selection of potential buildings was made on the RBD set of buildings coming with a construction year. The following table summarises the selection and manual validation: Area Potentials Metric Basel 450 EGIDs 209 EGIDs Bern 450 EGIDs 180 EGIDs Biasca 336 EGIDs 209 EGIDs Caslano 450 EGIDs 272 EGIDs The previous table gives the result of the second manual validation. Indeed, two manual validation sessions were made, with several weeks in-between, to check the validation process and how it evolved with the increase of the view of the problem. Three main critics can then be addressed to the metric: the first one is that establishing validation criterion is not simple as the number of cases in which buildings can fall is very high. Understanding the problem takes time and requires to see a lot of these cases. It then follows that the second validation session was more stable and rigorous than the first one. The second critic that can be made on our metric is the selection bias. As the process is made by a human, it is affected by its way of applying the criterion and more specifically on by its severity on their application. Considering the whole potentials sub-set, one can conclude that a few buildings could be rejected and validated depending on the person doing the selection. The last critic concerns specific cases for which the asynchronous criterion to reject them is weak. Indeed, for some buildings, the situation is very unclear in the way the RBD and the maps give information that can not be understood. This is the case for example when the building is not represented on the map. This can be the position in the RBD or the lack of information on the maps that lead to such an unclear situation. These cases are then rejected, but without being fully sure of the asynchronous aspect regarding the maps and the RBD . Methodology \u00b6 With a reliable metric, results can be assessed and the development of the proof-of-concept can be properly guided. As mentioned above, the proof-of-concept can be split in four major steps that are the processing of the maps, the extraction of the RBD buildings, detection of the building on the maps and detection in morphological changes. National Maps Processing In order to perform the detection of building on the maps, a reliable methodology is required. Indeed, one could perform the detection directly on the source maps but this would lead to a complicated process. Indeed, maps are mostly the result of the digitisation of paper maps creating a large number of artefacts on the digital images. This would lead to an unreliable way of detecting building as a complicated decision process would have to be implemented each time a RBD position is checked on each map. A map processing step was then introduced in the first place allowing to translate the color digitised images into reliable binary images on which building detection can be made safely and easily. The goal of this process is then to create a binary version of each map with black pixels indicating the building presence. A method of extracting buildings on maps was then designed. Considering the following example of a map cropped according to a defined geographical area ( Basel ): Example of a considered map: Basel in 2005 and closer view - Data: swisstopo The first step of the map processing methodology is to correct and standardise the exposure of the digitised maps. Indeed, as maps mostly result of a digitisation process, they are subjected to exposure variation due to the digitisation process. A simple standardisation is then applied. The next step consists in black pixel extraction. Each pixel of the input map is tested to determine whether or not it can be considered as black using specific thresholds. As the building are drawn in black, extracting black pixels is a first way of separating the buildings from the rest of the symbology. The following result is obtained: Result of the black extraction process As one can see on the result of the black extraction process, the buildings are still highly connected to other symbological elements and to each others in some cases. Having the building footprints well separated and well defined is an important point for subsequent processes responsible of construction years deduction. To achieve it, two steps are added. The first one uses a variation of the Conway game of life [5] to implement a morphological operator able to disconnect pixel groups. The following image gives the results of this separation process along with the previous black extraction result on which it is based: Result of the morphological operator (right) compare to the previous black extraction (left) As the morphological operator provides the desired result, it also shrinks the footprint of the elements. It allows to eliminate a lot of structures that are not buildings, but it also reduces the footprint of the buildings themselves, which can increase the amount of work to perform by the subsequent processes to properly detect a building. To solve this issue and to obtain building footprints that are as close as possible to the original map, a controlled re-growing step is added. It uses a region threshold and the black extraction result to re-grow the buildings without going any further of their original definition. The following images give a view of the final result along with the original map: Final result of the building footprints extraction (right) compared to the original map As the Conway morphological operator is not able to get rid of all the non-building elements, such as large and bold texts, the re-growing final step also thickening them along with the building footprints. Nevertheless, the obtained binary image is able to keep most of the building footprint intact while eliminating most of the other element of the map as illustrated on the following image: Extracted building footprints, in pink, superimposed on the Bern map The obtained binary images are then used for both detection of building and detection of morphological changes as the building are easy to access and to analyse on such representation. Building Extraction from RBD In the case of limited geographical areas as in this research project, extracting the relevant buildings from the RBD was straightforward. Indeed, the RBD is a simple DSV database that is very easy to understand and to process. The four areas were packed into a single DSV file and the relevant building were selected through a very simple geographical filtering. Each area being defined by a simple geographical square, selecting the buildings was only a question of checking if their position was in the square or not. Building Detection Process Based on the computed binary images, each area can be temporally covered with maps on which building can be detected. Thanks to the processed maps, this detection is made easily, as it was reduced to detect black pixels in a small area around the position of the building provided in the RBD . For each building in the RBD , its detection on each temporal version of the map is made to create a presence table of the building. Such table is simply a Boolean value indicating whether a building was there or not according to the position provided in the RBD . The following images give an illustration of the building detection process on a given temporal version of a selected map: Detection overlay superimposed on its original map (left) and on its binary counterpart (right) One can see that for each building and for each temporal version of the map, the decision of a building presence can be made. At the end of this process, each building is associated to a list of presence at each year corresponding to an available map. Morphological Change Detection Detecting the presence of a building on each temporal version of the map is a first step but is not enough to determine whether or not it is the desired building. Indeed, a building can be replaced by another along the time dimension without creating a discontinuity in the presence timeline. This would lead to misinterpret the presence of building with another one, leading the construction year to be deduced too far in time. This can be illustrated by the following example: Example of building being replaced by another one without introducing a gap in the presence table In case the detection of the presence of the building is not enough to correctly deduce a construction year, a morphological criterion is added. Many different methodologies have been tried in this project, going from signature to various quantities deduce out of the footprint of the building. The most simple and most reliable way was to focus on the pixel count of the building footprint, which corresponds to its surface in geographical terms. A morphological change is considered as the surface of the building footprint changes up to a given threshold along the building presence timeline. In such a case, the presence timeline is broken at the position of the morphological change, interpreting it in the same way as a formal appearance of a building. Introducing such criteria allowed to significantly improve our results, especially in the case of urban centers. Indeed, in modern cities, large number of new buildings were built just after a previous building was being destroyed due to the lack of spaces left for new constructions. Results \u00b6 The developed proof-of-concept is applied on the four selected areas to deduce construction year for each building appearing in the RBD . With the defined metric, it is possible to assess the result in a reliable manner. Nevertheless, assessing the results with clear representations is not straightforward. In this research project, two representations were chosen: Histogram of the success rate : For this representation, the building of the metric are assigned to temporal bins of ten years in size and the success rate of the construction year is computed for each bins. Distance and pseudo-distance distribution : As the previous representation only gives access to a binary view of the results, a distance representation is added to understand to which extent mistakes are made on the deduction of a construction year. For buildings detected between two maps, the temporal middle is assumed as the guessed construction year, allowing to compute a formal distance with its reference. In case a building is detected before or beyond the map range, a pseudo-distance of zero is assigned in case the result is correct according to the reference. Otherwise, the deduced year (that is necessarily between two maps) is compared to its reference extremal map date to obtain an error pseudo-distance. In addition to the manually defined metric, the full RBD metric is also considered. As the construction years provided in the RBD have to be considered with care, as part of them are incorrect, comparing the results obtained the full RBD metric and the metric we manually defined opens the important question of the synchronisation between the maps and the RBD , viewed from the construction perspective. Results: Basel Area The following figures give the Basel area result using the histogram representation. The left plot uses the full RBD metric while the right one uses the manually validated one: Histogram of the success rate - Ten years bins One can see one obvious element that is the result provided by the full RBD metric (left) and the manually validated metric (right) are different. This is a clear sign that the RBD and the maps are de-synchronised on a large fraction of the building set of Basel . The other element that can be seen on the right plot is that the deduction of the construction year are more challenging where maps are available. Indeed on the temporal range covered by the maps (vertical white lines), the results drops from the overall results to 50-60 % on some of the histogram bins. The following figures show the distance and pseudo-distance distribution of the error made on the deduced construction year according to the chosen metric: Distance (red and blue) and pseudo-distance (red) of the error on the construction years The same differences as previously observed between the two metrics can also be seen here. Another important observation is that the distribution seems mostly symmetrical. This indicates that no clear deduction bias can be observed in the results provided by the proof-of-concept. Results: Bern Area The following figures give the histogram view of the results obtained on the Bern area: Histogram of the success rate - Ten years bins One can observe that the results are similar to the result of Basel whilst being a bit better. In addition, one can clearly see that the difference between the full RBD metric and the manually validated metric huge here. This is probably the sign that the RBD is mostly incorrect in the case of Bern . The following figures show the distance distributions for the case of Bern : Distance (red and blue) and pseudo-distance (red) of the error on the construction years Again, the distribution of the error on the deduced construction year is symmetrical in the case of Bern . Results: Biasca Area The following figures give the histogram view of the success rate for the case of Biasca : Histogram of the success rate - Ten years bins In this case, the results are much better according to the manually validated metric. This can be explained by the fact that Biasca is a rural/mountainous area in which growing of the urban areas are much simpler as buildings once built tend to remain unchanged, limiting the difficulty to deduce a reliable construction year. The following figures show the distance distribution for Biasca : Distance (red and blue) and pseudo-distance (red) of the error on the construction years This confirms the results seen on the histogram figure and shows that the results are very good on such areas. Results: Caslano Area Finally, the following figures show the histogram view of the success rate of the proof-of-concept on the case of Caslano : Histogram of the success rate - Ten years bins The same consideration applies as for the Biasca case. The results are very good as part of the Caslano area can be considered as rural or at least peri-urban. The results are a bit less good than in the Biasca case, drawing the picture that urban centres are more difficult to infer than rural areas. The following figures show the error distribution for Caslano : Distance (red and blue) and pseudo-distance (red) of the error on the construction years Results: Synthesis In order to synthesise the previous results, that were a bit dense due to the consideration of two representations and two metrics, the following summary is given: Basel : 78.0% of sucess rate and 80.4% of building correctly placed within \u00b15.5 years Bern : 84.4% of sucess rate and 85.0% of building correctly placed within \u00b15.6 years Biasca : 93.5% of sucess rate and 93.9% of building correctly placed within \u00b16.0 years Caslano : 90.8% of sucess rate and 91.2% of building correctly placed within \u00b16.2 years These results only consider the manually validated metric for all of the four areas. By weighting each area with their amount of buildings, one can deduce the following numbers: Switzerland : 83.9% of success rate and 84.7% of building correctly place within \u00b15.8 years These last numbers can be considered as a reasonable extrapolation of the proof-of-concept performance on the overall Switzerland. Conclusion \u00b6 As a main conclusion to the national maps approach, one can consider the results as good. It was possible to develop a proof-of-concept and to apply it on selected and representative areas of Switzerland. In this approach, it turns out that developing the proof-of-concept was the easy part. Indeed, finding a metric and demonstrating its representativeness and reliability was much more complicated. Indeed, as the two datasets can not be considered as fully reliable in the first place, a strategy had to be defined in order to be able to demonstrate that the chosen metric was able to assess our result in the way expected by the Statistical Office . In addition, the metric only required one additional hypothesis on top of the two datasets. This hypothesis, consisting in assuming that the synchronous sub-set was a quasi-sub-set of the absolutely correct construction years , can be assumed to be reasonable. Nevertheless it is important to emphasise that it was necessary to make it, leading us to remains critic and careful whilst reading the results given by our metric. The developed proof-of-concept was developed in C++ , leading to an efficient code able to be used for the whole processing of Switzerland without the necessity to deeply modify it. Research Approach: Statistical \u00b6 As the availability of the topographic/national maps does not reach the integrity of all building's year of construction in the registry, an add-on was developed to infer this information, whenever there was this need for extrapolation. Usually, the maps availability reaches the 1950s, whilst in some cities the minimum year of construction can be in the order of the 12th century, e.g. The core of this statistical model is based on the Concentric Zones Model (Park and Burgess, 1925)[6] extended to the idea of the growth of the city from the a centre (Central Business District - CBD) to all inner areas. The concept behind this statistical approach can be seen below using the example of a crop of Basel city: Illustration of the Burgess concentric zone model Although it is well known the limits of this model, which are strongly described in other famous urban models such as from Hoyt (1939)[7] and Harris and Ullman (1945)[8]. In general those critics refer to the simplicity of the model, which is considered and compensated for this application, especially by the fact that the main prediction target are older buildings that are assumed to follow the concentric zones pattern, differently than newer ones (Duncan et al., 1962)[9]. Commonly this is the pattern seen in many cities, hence older buildings were built in these circular patterns to some point in time when reconstructions and reforms are almost randomly placed in spatial and temporal terms. Moreover processes like gentrification are shown to be dispersed and quite recent (R\u00e9rat et al, 2010)[10]. In summary, a first predictor is built on the basis that data present a spatial dependence, as in many geostatistical models (Kanevski and Maignan, 2004[11]; Diggle and Ribeiro, 2007[12]; Montero and Mateu, 2015[13]). This way we are assuming that closer buildings are more related to distant buildings (Tobler, 1970[14]) in terms of year of construction and ergo the time dimension is being interpolated based on the principles of spatial models. We are here also demonstrating how those two dimensions interact. After that concentric zones are embedded through the use of quantiles, which values will be using in a probabilistic unsupervised learning technique. Finally, the predicted years are computed from the clusters generated. Metric \u00b6 Similar to the detection situation, generating a validation dataset was an especially challenging task. First of all, the dates in the RBD database could not be trusted in their integrity and the topographic maps used did not reach this time frame. In order to ascertain the construction year in the database, aerial images from swisstopo (Swiss Federal Office of Topography) were consulted and this way buildings were manually selected to compound a validation dataset. References extraction from aerial images manual analysis One of the problems related to this approach was the fact that a gap between the surveys necessary for the images exists. This way it is not able to state with precision the construction date. These gaps between surveys were approximately in the range of 5 years, although in Basel , for some areas, it reached 20 years. An example of this methodology to create a trustworthy validation set can be seen below. In the left-hand side one can see the year of the first image survey (up) and the year registered in the RBD (down) and in the right-hand side, one can see the year of the next image survey in the same temporal resolution. Methodology \u00b6 First of all, a prior searching radius is defined as half of the largest distance (between random variables). For every prediction location, the variance between all points in the prior searching radius will be used to create a posterior searching radius. This way, the higher the variance, the smaller the searching radius, as we tend to trust data less. This is mainly based on the principle of spatial dependence used in many geostatistical interpolators. The exception to this rule is for variances that are higher than 2 x the mean distance between points. In this case, the searching radius increases again in order to avoid clusters of very old houses that during tests caused underestimation. The figure below demonstrates the logic being the creation of searching radii. Searching radii computation process being d the distance between points, \u03bc the mean and s\u00b2 the variance of random variable values within the prior searching radius. It is important to mention that in case of very large number of missing data, if the searching radius does not find enough information, the posterior mean will be the same as the prior mean, possibly causing over/underestimation in those areas. This first procedure is used to fill the gaps in the entry database so clustering can be computed. The next step is then splitting the data in 10 quantiles, what could give the idea of concentric growth zones, inspired, in Burgess Model (1925)[7]. Every point in the database will then assume the value of its quantile. It is also possible to ignore this step and pass to clustering directly, what can be useful in two situations, if a more general purpose is intended or if the concentric zones pattern is not observed in the study area. As default, this step is used, which will be followed by an unsupervised learning technique. A gaussian mixture model, which does not only segments data into clusters, but indicates the probability of each point belonging to every cluster is then performed. The number of components computed is a linear function to the total number of points being used, including the ones that previously had gaps. The function to find the number of components is the following: being np the number of components/clusters, and nc the total number of points used. The number of clusters shall usually be very large compared to a standard clustering exercise. To avoid this, this value is being divided by ten, but the number of clusters will never be smaller than five. An example of clustering performed by the embedded gaussian mixture model can be seen below: Example of clustering process on the Basel area Hence the matrix of probabilities of every point belonging to each cluster (\u03bb - what can be considered a matrix of weights) is multiplied by the mean of each cluster ( 1 x nc matrix mc ), forming the A matrix: or in matrices: Finally, the predictions can then be made using the sum of each row in the A matrix. It is important to state that the same crops (study areas) were used for this test. Although Caslano was not used in this case, as it possesses too few houses with a construction date below the oldest map available. Using the metric above explained a hold out cross-validation was performed, this way a group of points was only used for validation and not for training. After that, the RMSE (Root Mean Squared Error) was calculated using the difference between the date in the RBD database and the predicted one. This RMSE was also extrapolated to the whole Switzerland, so one could have a notion of what the overall error could be, using the following equation (for the expected error): where E is the error and n the number of buildings in each region. In addition to the RMSE , the 95th percentile was computed for every study area and using all combined as well. Hence, one could discuss the spread and predictability of errors. Results \u00b6 The first case analysed was Basel , where the final RMSE was 9.78 years. The density plot below demonstrates the distribution of errors in Basel , considering the difference between the year of construction in the RBD database and the predicted one. Distribution of error on construction year extrapolation Among the evaluated cases, Basel presented a strong visible spatial dependence, and it was also the case which the largest estimated proportion of houses with construction years older than (1955) the oldest map ( 11336 or approximately 66 % of buildings). Based on the validation dataset only, there was an overall trend of underestimation and the 95th percentile reached was 20 years, showing a not so spread and flat distribution of errors. Bern was the second case evaluated, and it demonstrated to be an atypical case. This starts from the fact that a big portion of the dates seemed incongruent with reality, based on the aerial images observed and as seen in the previous detection approach. Not only that, but almost 80 % of the buildings in Bern had missing data to what refers to the year of construction. This is especially complicated as the statistical method here presented is in essence an interpolator (intYEARpolator). Basically, as in any inference problem, data that is known is used to fill unknown data, therefore a reasonable split among known and unknown inputs is expected, as well as a considerable confidence on data. In the other hand, an estimated number of 1079 (approximately 27 % of the buildings) buildings was probably older than the oldest map available (1954) in Bern crop. Therefore, in one way liability was lower in this case, but the number of prediction points was smaller too. The following figure displays the density of errors in Bern, where an RMSE of 20.64 years was computed. Distribution of error on construction year extrapolation There was an overall trend for overestimation, though there was still enough lack of spread in errors, especially if one considers the 95th percentile of 42 . Finally, the crop on Biasca was evaluated. The computed RMSE was of 13.13 years, which is closer to the Basel case and the 95th percentile was 17 years, this way presenting the least spread error distribution. In Biasca an estimated 1007 ( 32 %) buildings were found, which is not much more than the proportion in Bern, but Biasca older topographic map used was from 1970, making of it an especially interesting case. The density plot below demonstrates the concentrated error case of Biasca : Distribution of error on construction year extrapolation Once the RMSE was computed for the three regions, it was extrapolated to the whole Switzerland by making consideration the size of each dataset: Extrapolation of the error distribution on the whole Switzerland The expected extrapolated error calculated was 15.6 years and the 95th percentile was then 31 years. Conclusion \u00b6 This add-on allows extrapolating the predictions to beyond the range of the topographical maps. Its predictions are limited, but the accuracy reached can be considered reasonable, once there is a considerable lack of information in this prediction range. Nor the dates in the RBD , nor the topographic maps can be fully trusted, ergo 15.6 years of error for the older buildings is acceptable, especially by considering the relative lack of spread in errors distribution. If a suggestion for improvement were to be given, a method for smoothing the intYEARpolator predictions could be interesting. This would possibly shift the distribution of the error into closer to a gaussian with mean zero. The dangerous found when searching for such an approach is that the year of construction of buildings does not seem to present a smooth surface, despite the spatial dependence. Hence, if this were to be considered, a balance between smoothing and variability would need to found. We also demonstrated a completely different perspective on how the spatial and temporal dimensions can be joined as the random variable predicted through spatial methodology was actually time. Therefore a strong demonstration of the importance of time in spatially related models and approaches was also given. The code for the intYEARpolator was developed in Python and it runs smoothly even with this quite big proportion of data. The singular case it can be quite time-demanding is in the case of high proportion of prediction points (missing values). It should also be reproducible to the whole Switzerland with no need for modification. A conditional argument is the use of concentric zones, that can be excluded in case of a total different pattern of processing time. Reproduction Resources \u00b6 The source code of the proof-of-concept for national maps can be found here : National maps approach proof-of-concept (regbl-poc), STDL The README provides all the information needed to compile and use the proof-of-concept. The presented results and plots can be computed using the following tools suite : National maps approach results and plots (regbl-poc-analysis), STDL with again the README giving the instructions. The proof-of-concept source code for the statistical approach can be found here : Statistical approach proof-of-concept (regbl-poc-intyearpolator), STDL with its README giving the procedure to follow. The data needed to reproduce the national maps approach are not publicly available. For the national maps, a temporal series of the 1:25'000 maps of the same location are needed. They can be asked to swisstopp : GeoTIFF National Maps 1:25'000 rasters temporal sequence, swisstopo With the maps, you can follow the instruction for cutting and preparing them on the proof-of-concept README . The RBD data, used for both approaches, are not publicly available either. You can query them using the request form on the website of the Federal Statistical Office : DSV RBD data request form , Federal Statistical Office Both proof-of-concepts READMEs provide the required information to use these data. References \u00b6 [1] Federal Statistical Office [2] Federal Register of Buildings and Dwellings [3] Federal Office of Topography [4] National Maps (1:25'000) [5] Conway, J. (1970), The game of life. Scientific American, vol. 223, no 4, p. 4. [6] Park, R. E.; Burgess, E. W. (1925). \"The Growth of the City: An Introduction to a Research Project\". The City (PDF). University of Chicago Press. pp. 47\u201362. ISBN 9780226148199. [7] Hoyt, H. (1939), The structure and growth of residential neighborhoods in American cities (Washington, DC). [8] Harris, C. D., and Ullman, E. L. (1945), \u2018The Nature of Cities\u2019, Annals of the American Academy of Political and Social Science, 242/Nov.: 7\u201317. [9] Duncan, B., Sabagh, G., & Van Arsdol,, M. D. (1962). Patterns of City Growth. American Journal of Sociology, 67(4), 418\u2013429. doi:10.1086/223165 [10] R\u00e9rat, P., S\u00f6derstr\u00f6m, O., Piguet, E., & Besson, R. (2010). From urban wastelands to new\u2010build gentrification: The case of Swiss cities. Population, Space and Place, 16(5), 429-442. [11] Kanevski, M., & Maignan, M. (2004). Analysis and modelling of spatial environmental data (Vol. 6501). EPFL press. [12] Diggle, P. J. Ribeiro Jr., P. J. (2007). Model-based Geostatistics. Springer Series in Statistics. [13] Montero, J. M., & Mateu, J. (2015). Spatial and spatio-temporal geostatistical modeling and kriging (Vol. 998). John Wiley & Sons. [14] Tobler, W. R. (1970). A computer movie simulating urban growth in the Detroit region. Economic geography, 46(sup1), 234-240.","title":"COMPLETION OF THE FEDERAL REGISTER OF BUILDINGS AND DWELLINGS"},{"location":"TASK-REGBL/#completion-of-the-federal-register-of-buildings-and-dwellings","text":"Nils Hamel (UNIGE) - Huriel Reichel (swisstopo) Proposed by the Federal Statistical Office - TASK-REGBL December 2020 to February 2021 - Published on March 2, 2021 Abstract : The Swiss Federal Statistical Office is in charge of the national Register of Buildings and Dwellings (RBD) which keeps track of every existing building in Switzerland. Currently, the register is being completed with buildings in addition to regular dwellings to offer a reliable and official source of information. The completion of the register introduced issues due to missing information and their difficulty to be collected. The construction year of the buildings is one missing information for a large amount of register entries. The Statistical Office mandated the STDL to investigate on the possibility to use the Swiss National Maps to extract this missing information using an automated process. A research was conducted in this direction with the development of a proof-of-concept and a reliable methodology to assess the obtained results.","title":"COMPLETION OF THE FEDERAL REGISTER OF BUILDINGS AND DWELLINGS"},{"location":"TASK-REGBL/#introduction","text":"The Swiss Federal Statistical Office [1] is responsible of maintaining the Federal Register of Buildings and Dwellings ( RBD ) in which a collection of information about buildings and homes are stored. Currently, a completion operation of the register is being conducted to include to it any type of construction on the Swiss territory. Such completion operation comes with many challenges including the gathering of the information related to the construction being currently integrated to the register. In this set of information are the construction years of the buildings. Such information is important to efficiently characterise each Swiss building and to allow the Statistical Office to provide a reliable register to all actors relying on it. The construction year of buildings turns out to be complicated to gather, as adding new buildings to the register already impose an important workload even for the simple information. In addition, in many cases, the construction year of the building is missing or can not be easily collected to update the register. The Statistical Office mandated the STDL to perform researches on the possibility to automatically gather the construction year by analysing the swisstopo [3] National Maps [4]. Indeed, the Swiss national maps are known for their excellency, their availability on any geographical area, and for their temporal cover. The national maps are made with a rigorous and well controlled methodology from the 1950s and therefore they can be used as a reliable source of information to determine the buildings' construction year. The STDL was then responsible for performing the researches and developing a proof-of-concept to provide all the information needed to the Statistical Office for them to take the right decision on considering national maps as a reliable way of assigning a construction year for the buildings lacking information.","title":"Introduction"},{"location":"TASK-REGBL/#research-project-specifications","text":"Extracting the construction date out of the national maps is a real challenge, as the national maps are a heavy dataset, they are not easy to be considered as a whole. In addition, the Statistical Office needs the demonstration that it can be done in a reliable way and within a reasonable amount of time to limit the cost of such process. They are also subjected to strict tolerances on the efficiency of the construction years extraction through an automated process. The goal of at least 80 % of overall success was then provided as a constraint to the STDL . As a result, the research specifications for the STDL were: Gathering and understanding the data related to the problem Developing a proof-of-concept demonstrating the possibility to extract the construction years from the national maps Assessing the results with a reliable metric to allow demonstrating the quality and reliability of the obtained construction years","title":"Research Project Specifications"},{"location":"TASK-REGBL/#research-data-selected-areas","text":"In this research project, two datasets were considered: the building register itself and the national maps. As both datasets are heavy and complex, considering them entirely for such a research project would have been too complicated and unnecessary. It was then decided to focus on four areas selected for their representativeness of Swiss landscape: Basel (BS): Urban area Bern (BE): Urban and peri-urban area Biasca (TI): Rural and mountainous Caslano (TI): Peri-urban and rural The following images give a geographical illustration of the selected areas through their most recent map: Illustration of the selected areas: Basel (2015), Bern (2010), Biasca (2012) and Caslano (2009) Data: swisstopo Basel was selected as it was one example of an area on which the building register was already well filled in terms of construction years. The four regions are 6km by 6km squared areas which allows up to twenty thousand buildings to be considered on a single one.","title":"Research Data &amp; Selected Areas"},{"location":"TASK-REGBL/#federal-register-of-buildings-and-dwellings","text":"The register of buildings is a formal database composed with entries, each of them representing a specific building. Each entry comes with a set of information related to the building they describe. In this project, a sub-set of these informations was considered: Federal identifier of the building (EGID) The position of the building, expressed in the EPGS:2056 (GKODE, GKODN) The building construction year, when available (GBAUJ) The surface of the building, when available, expressed in square metres (GAREA) In addition, tests were conducted by considering the position of the entries of each building. In turned out rapidly that they were not useful in this research project as they were missing on a large fraction on the register and only providing a redundant information according to the position of the buildings. The following table gives a summary of the availability of the construction year in the register according to the selected areas: Area Buildings Available years Missing fraction Basel 17\u2019088 16\u2019584 3% Bern 21\u2019251 4\u2019499 79% Biasca 3\u2019774 1\u2019346 64% Caslano 5\u2019252 2\u2019452 53% One can see that the amount of missing construction year can be large depending on the considered area.","title":"Federal Register of Buildings and Dwellings"},{"location":"TASK-REGBL/#national-maps","text":"On the side of the national maps, the dataset is more complex. In addition to the large number of available maps, variations of them can also be considered. Indeed, maps are made for different purposes and come with variations in their symbology to emphasise elements on which they focus. Moreover, for modern years, sets of vector data can also be considered in parallel to maps. Vector data are interesting as they allow to directly access the desired information, that is the footprint of the building without any processing required. The drawback of the vector data is their temporal coverage which is limited to the last ten to twenty years. The following images give an illustration of the aspect of the available maps and vector datasets considering the example of the Bern area. Starting with the traditional maps : Available map variations: KOMB, KGRS and KREL - Data: swisstopo and the more specific and vector ones: Available map variations: SITU, GEB and DKM25-GEB (vector) - Data: swisstopo In addition to the number of available variations and data types, they all come with their specific temporal coverage. In the case of this research project, we tried to go back in time as much as possible, simplifying the choice for the older maps. The question still remains for more modern times. As we are mostly interesting in buildings, the availability of already extracted building layers, that can be either raster or vector data, is highly interesting. But the problem of data selection is complex in our case. Indeed, no matter the choice, on the older times, the only available maps have to be considered. In addition to building footprint access, the question of the continuity of the data as to be considered with care. More than building footprints, we are interested in the continuity of these footprints, in order to be able to safely assume the cycle of life of the tracker buildings. This consideration led us to discover variation in methodologies depending on the considered set of data. Indeed, buildings are not shaped in the same way on traditional maps than they are in layer focusing on them. It follows that variation of the symbology, so do the shape of the building, appears between traditional maps and building layers (raster and vector). These variations can lead to shocks going from a map to the one preceding it in time. This can break the continuity of the building footprints along time, making them much more difficult to track safely. This is the reason we choose to focus on the KOMB variation of the maps. These maps are very stable and covers the largest temporal ranges. The methodology was kept very similar along the years, making this dataset much more reliable to work with when the time dimension is considered. Only considering the KOMB variation of the maps also allows to ensure that all source data is treated the same in the processing pipeline, easing the assessment of the results. In addition, the KOMB maps are dense in information and come with colorimetry of their symbology. This opens the possibility to more easily extract the information we need in this project, that are the building footprints. One exception was made concerning the KOMB maps: in their very latest version, the methodology changed, causing the symbology to be different with the older KOMB maps. In their latest version, texts are much more numerous and tend to cover a large amount of the buildings, making them invisible . For this reason, their latest version was dropped, slightly reducing the temporal coverage on the 2015-2020 period. Selecting the KOMB variation allowed us to obtain the following temporal coverage for the four selected areas: Area Oldest map Latest map Mean separation Basel 1955 2015 5.5 Years Bern 1954 2010 5.6 Years Biasca 1970 2012 6.0 Years Caslano 1953 2009 6.2 Years One can see that a large portion of the 20th century can be covered using the maps with a very good resolution of around five to six years between the maps.","title":"National Maps"},{"location":"TASK-REGBL/#research-approaches","text":"In this research project, the main focus was put on the national maps to extract the construction year of buildings as the maps are sources on which we can rely and assess the results. The only drawback of the maps is their limited temporal coverage, as they only start to be available in the 1950s . This is the reason why another experimental approach was also added to address the cases of building being built before the 1950s . This secondary approach focused on a statistical methodology to verify to which extent it could be possible to assign a construction date even in the case no maps are available. National Maps : This main approach focuses on the national maps from which the construction year of a building is deduced from a temporal analysis of the maps. Each building is tracked until it disappears or change its shape on a given map allowing to deduce that the construction of the building was made in the gap separating the map and its successor one. Statistical Analysis : This method is based on the principle of spatial dependence and furthermore on concentric zones of urban development. This is technically an interpolator which deduces construction years based first on different searching radii for difference variances, second by splitting the data in quantiles and, finally, by a gaussian mixture model unsupervised learning technique to gather the final predictions. The statistical analysis allows then to consider buildings that were detected on all maps, meaning their construction is older than the oldest available map, to assign them an estimation of their construction year, knowing they had to be older than the oldest map.","title":"Research Approaches"},{"location":"TASK-REGBL/#research-approach-national-maps","text":"In order to detect construction year of buildings, we need to be able to track them down on the maps across the temporal coverage. The RBD is providing the reference list of the building, each coming with a federal identifier ( EGID ) and a position. This position can then be used to track down the building on maps for its appearance or morphological change. As the maps are already selected, as the research areas, this research approach can be summarised in the following way: Translating maps into binary images containing only building Extracting the RBD buildings related to the analysed area Detection procedure of the buildings on the maps Detection of the morphological variation of the buildings Assessment of the obtained results The four first points are related to the development of the proof-of-concept. The last one concern a very sensitive and complicated question relative to the considered problem: how to analyse and assess the obtained results. This question was to most difficult question in this research, and finding a clear and reliable answer is mandatory before to develop anything. For this reason, it is considered in the first place.","title":"Research Approach: National Maps"},{"location":"TASK-REGBL/#reliability-of-the-data","text":"Assessing the results is essentially having a strong reference allowing to compare both in order to obtain a reliable characterisation of the success rate in the deduction of the construction years. This question leads to the discovery that this problem is much more complex that and can appear in the first place. Indeed, we were warned by the Statistical Office that the RBD , considering the construction years it already gives, can be unreliable on some of its portions. This can be explained by the fact that collecting such information is a long and complicated administrative process. As an example, the following image gives an illustration of a building tracked on each of the available selected maps: Temporal track of a selected building On this illustration, one can see two things: the RBD announce a construction year in 1985 ; the maps are clearly indicating something different, locating its construction year between 1963 and 1969 . So both datasets are contradicting each other. In order to solve the contradiction, we manually searched for historical aerial images. The following images illustrate what was found: Aerial view of the building situation: 1963, 1967 and 1987 - Data: swisstopo One can clearly see that the maps seem to give the correct answer concerning the construction date of this specific building, the RBD being contradicted by two other sources. This illustrates the fact that the RBD can not be directly considered as a reliable reference to assess the results. The same question applies for the maps. Even if it is believed that they are highly reliable, one has to be careful with such consideration. Indeed, looking at the following example: Temporal track of a selected building In this case, the RBD gives 1986 as the construction date of the pointed building. The maps are giving a construction year between 1994 and 2000 . Again, the two datasets are contradicting each other. The same procedure was conducted to solve the contradiction: Aerial view of the building situation: 1970, 1986 and 1988 - Data: swisstopo Looking at the aerial images, it seems that the tracked building was there in 1988 . One can see that the map in 1994 continue to represent the four old buildings instead on the new one. It's only in 2000 that the maps are correctly representing the new building. This shows that despite maps are a reliable source of geo-information, they can also be subjected to delay in their symbology. The maps also come with the problem of the consistency of the building footprint symbology. Looking at the following example: Temporal track of a selected building one can see that the maps seem to indicate a strange evolution of the situation: a first building appears in 1987 and it is destroyed and replaced by a larger one in 1993 . Then, this new large building seems to have been destroyed right after its construction to be replaced by a new one in 1998 . Considering aerial images of the building situation: Aerial image view of the building situation: 1981, 1987 and 1993 - Data: swisstopo one can clearly see that a first building was constructed and completed by an extension between 1987 and 1993 . This shows an illustration where the symbology of the building footprints can be subjected to variation than can be de-synchronised regarding the true situation.","title":"Reliability of the Data"},{"location":"TASK-REGBL/#metric","text":"In such context, neither the RBD or the national maps can be formally considered as a reference. It follows that we are left without a solution to assess our results, and more problematically, without any metric able to guide the developments of the proof-of-concept in the right direction. To solve the situation, one hypothesis is made in this research project. Taking into account both the RBD and the national maps, one can observe that both are built using methodologies that are very different. On one hand, the RBD is built out of a complex administrative process, gathering the required information in a step by step process, going from communes to cantons , and finally to the Statistical Office . On the other hand, the national maps are built using regular aerial image campaigns conducted over the whole Switzerland. The process of establishing maps is quite old and can then be considered as well controlled and stable. Both datasets are then made with methodologies that can be considered as fully independent from each other. This led us to the formulation of our hypothesis: Hypothesis : As the RBD and national maps are the results of independent methodologies, an error in one dataset is very unlikely to compensate an error in the other. In other words, if the RBD and the national maps agree on the construction year of a building, this information can be considered as a reliable reference, as it would be very unlikely to have two errors leading to such agreement. One should remain careful with this hypothesis, despite it sounds reasonable. It would be very difficult to assess it as requiring to gather complex confirmation data that would have to be independent of the RBD , the national maps and the aerial images (as maps are based on them). This assumption is the only one made in this research project. Accepting this assumption leads us to the possibility to establish a formal reference that can be used as a metric to assess the results and to guide the development of the proof-of-concept. But such reference has to be made with care, as the problem remains complex. To illustrate this complexity, the following figure gives a set representation of our problem: Set representation of the RBD completion problem The two rectangles represent the set of buildings for a considered area. On the left, one can see the building set from the RBD point of view. The grey area shows the building without the information of their construction year. Its complementary set is split in two sub-sets that are the buildings having a construction year that is absolutely correct and absolutely incorrect (the limit between both is subject to a bit of interpretation, as the construction year is not a strong concept). If a reference can be extracted, it should be in the green sub-set. The problem is that we have no way of knowing which building are in which sub-set. So the national maps were considered to define another sub-set: the synchronous sub-set where both RBD and national maps agree. To build the metric, the RBD sub-set of buildings coming with the information of the construction year is randomly sub-sampled to extract a representative sub-set: the potentials . This sub-set of potentials is then manually analysed to separate the building on which both datasets agree and to reject the other. At the end of the process, the metric sub-set is obtained and should remain representative. On the right of the set representation is the view of the buildings set through the national maps. One can see that the same sub-set appears but it replaces the construction years by the representation of the building on the maps. The grey part is then representing the building that are not represented on the maps because of their size or because they can be hidden by the symbology for example. The difference is that the maps do not give access to the construction years directly, but they are read from the maps through our developed detector. The detector having a success rate, it cuts the whole set of sub-sets in half, which is exactly what we need for out metric. If the metric sub-set remains representative, the success rate of the detector evaluated on it should generalise to the whole represented buildings. This set representation demonstrates that the problem is very complex and has to be handled with care. Considering only the six most important sub-set and considering construction year are extracted by the detector from the maps, it means that up to 72 specific case can apply on each building randomly selected. To perform the manual selection, a random selection of potential buildings was made on the RBD set of buildings coming with a construction year. The following table summarises the selection and manual validation: Area Potentials Metric Basel 450 EGIDs 209 EGIDs Bern 450 EGIDs 180 EGIDs Biasca 336 EGIDs 209 EGIDs Caslano 450 EGIDs 272 EGIDs The previous table gives the result of the second manual validation. Indeed, two manual validation sessions were made, with several weeks in-between, to check the validation process and how it evolved with the increase of the view of the problem. Three main critics can then be addressed to the metric: the first one is that establishing validation criterion is not simple as the number of cases in which buildings can fall is very high. Understanding the problem takes time and requires to see a lot of these cases. It then follows that the second validation session was more stable and rigorous than the first one. The second critic that can be made on our metric is the selection bias. As the process is made by a human, it is affected by its way of applying the criterion and more specifically on by its severity on their application. Considering the whole potentials sub-set, one can conclude that a few buildings could be rejected and validated depending on the person doing the selection. The last critic concerns specific cases for which the asynchronous criterion to reject them is weak. Indeed, for some buildings, the situation is very unclear in the way the RBD and the maps give information that can not be understood. This is the case for example when the building is not represented on the map. This can be the position in the RBD or the lack of information on the maps that lead to such an unclear situation. These cases are then rejected, but without being fully sure of the asynchronous aspect regarding the maps and the RBD .","title":"Metric"},{"location":"TASK-REGBL/#methodology","text":"With a reliable metric, results can be assessed and the development of the proof-of-concept can be properly guided. As mentioned above, the proof-of-concept can be split in four major steps that are the processing of the maps, the extraction of the RBD buildings, detection of the building on the maps and detection in morphological changes.","title":"Methodology"},{"location":"TASK-REGBL/#results","text":"The developed proof-of-concept is applied on the four selected areas to deduce construction year for each building appearing in the RBD . With the defined metric, it is possible to assess the result in a reliable manner. Nevertheless, assessing the results with clear representations is not straightforward. In this research project, two representations were chosen: Histogram of the success rate : For this representation, the building of the metric are assigned to temporal bins of ten years in size and the success rate of the construction year is computed for each bins. Distance and pseudo-distance distribution : As the previous representation only gives access to a binary view of the results, a distance representation is added to understand to which extent mistakes are made on the deduction of a construction year. For buildings detected between two maps, the temporal middle is assumed as the guessed construction year, allowing to compute a formal distance with its reference. In case a building is detected before or beyond the map range, a pseudo-distance of zero is assigned in case the result is correct according to the reference. Otherwise, the deduced year (that is necessarily between two maps) is compared to its reference extremal map date to obtain an error pseudo-distance. In addition to the manually defined metric, the full RBD metric is also considered. As the construction years provided in the RBD have to be considered with care, as part of them are incorrect, comparing the results obtained the full RBD metric and the metric we manually defined opens the important question of the synchronisation between the maps and the RBD , viewed from the construction perspective.","title":"Results"},{"location":"TASK-REGBL/#conclusion","text":"As a main conclusion to the national maps approach, one can consider the results as good. It was possible to develop a proof-of-concept and to apply it on selected and representative areas of Switzerland. In this approach, it turns out that developing the proof-of-concept was the easy part. Indeed, finding a metric and demonstrating its representativeness and reliability was much more complicated. Indeed, as the two datasets can not be considered as fully reliable in the first place, a strategy had to be defined in order to be able to demonstrate that the chosen metric was able to assess our result in the way expected by the Statistical Office . In addition, the metric only required one additional hypothesis on top of the two datasets. This hypothesis, consisting in assuming that the synchronous sub-set was a quasi-sub-set of the absolutely correct construction years , can be assumed to be reasonable. Nevertheless it is important to emphasise that it was necessary to make it, leading us to remains critic and careful whilst reading the results given by our metric. The developed proof-of-concept was developed in C++ , leading to an efficient code able to be used for the whole processing of Switzerland without the necessity to deeply modify it.","title":"Conclusion"},{"location":"TASK-REGBL/#research-approach-statistical","text":"As the availability of the topographic/national maps does not reach the integrity of all building's year of construction in the registry, an add-on was developed to infer this information, whenever there was this need for extrapolation. Usually, the maps availability reaches the 1950s, whilst in some cities the minimum year of construction can be in the order of the 12th century, e.g. The core of this statistical model is based on the Concentric Zones Model (Park and Burgess, 1925)[6] extended to the idea of the growth of the city from the a centre (Central Business District - CBD) to all inner areas. The concept behind this statistical approach can be seen below using the example of a crop of Basel city: Illustration of the Burgess concentric zone model Although it is well known the limits of this model, which are strongly described in other famous urban models such as from Hoyt (1939)[7] and Harris and Ullman (1945)[8]. In general those critics refer to the simplicity of the model, which is considered and compensated for this application, especially by the fact that the main prediction target are older buildings that are assumed to follow the concentric zones pattern, differently than newer ones (Duncan et al., 1962)[9]. Commonly this is the pattern seen in many cities, hence older buildings were built in these circular patterns to some point in time when reconstructions and reforms are almost randomly placed in spatial and temporal terms. Moreover processes like gentrification are shown to be dispersed and quite recent (R\u00e9rat et al, 2010)[10]. In summary, a first predictor is built on the basis that data present a spatial dependence, as in many geostatistical models (Kanevski and Maignan, 2004[11]; Diggle and Ribeiro, 2007[12]; Montero and Mateu, 2015[13]). This way we are assuming that closer buildings are more related to distant buildings (Tobler, 1970[14]) in terms of year of construction and ergo the time dimension is being interpolated based on the principles of spatial models. We are here also demonstrating how those two dimensions interact. After that concentric zones are embedded through the use of quantiles, which values will be using in a probabilistic unsupervised learning technique. Finally, the predicted years are computed from the clusters generated.","title":"Research Approach: Statistical"},{"location":"TASK-REGBL/#metric_1","text":"Similar to the detection situation, generating a validation dataset was an especially challenging task. First of all, the dates in the RBD database could not be trusted in their integrity and the topographic maps used did not reach this time frame. In order to ascertain the construction year in the database, aerial images from swisstopo (Swiss Federal Office of Topography) were consulted and this way buildings were manually selected to compound a validation dataset. References extraction from aerial images manual analysis One of the problems related to this approach was the fact that a gap between the surveys necessary for the images exists. This way it is not able to state with precision the construction date. These gaps between surveys were approximately in the range of 5 years, although in Basel , for some areas, it reached 20 years. An example of this methodology to create a trustworthy validation set can be seen below. In the left-hand side one can see the year of the first image survey (up) and the year registered in the RBD (down) and in the right-hand side, one can see the year of the next image survey in the same temporal resolution.","title":"Metric"},{"location":"TASK-REGBL/#methodology_1","text":"First of all, a prior searching radius is defined as half of the largest distance (between random variables). For every prediction location, the variance between all points in the prior searching radius will be used to create a posterior searching radius. This way, the higher the variance, the smaller the searching radius, as we tend to trust data less. This is mainly based on the principle of spatial dependence used in many geostatistical interpolators. The exception to this rule is for variances that are higher than 2 x the mean distance between points. In this case, the searching radius increases again in order to avoid clusters of very old houses that during tests caused underestimation. The figure below demonstrates the logic being the creation of searching radii. Searching radii computation process being d the distance between points, \u03bc the mean and s\u00b2 the variance of random variable values within the prior searching radius. It is important to mention that in case of very large number of missing data, if the searching radius does not find enough information, the posterior mean will be the same as the prior mean, possibly causing over/underestimation in those areas. This first procedure is used to fill the gaps in the entry database so clustering can be computed. The next step is then splitting the data in 10 quantiles, what could give the idea of concentric growth zones, inspired, in Burgess Model (1925)[7]. Every point in the database will then assume the value of its quantile. It is also possible to ignore this step and pass to clustering directly, what can be useful in two situations, if a more general purpose is intended or if the concentric zones pattern is not observed in the study area. As default, this step is used, which will be followed by an unsupervised learning technique. A gaussian mixture model, which does not only segments data into clusters, but indicates the probability of each point belonging to every cluster is then performed. The number of components computed is a linear function to the total number of points being used, including the ones that previously had gaps. The function to find the number of components is the following: being np the number of components/clusters, and nc the total number of points used. The number of clusters shall usually be very large compared to a standard clustering exercise. To avoid this, this value is being divided by ten, but the number of clusters will never be smaller than five. An example of clustering performed by the embedded gaussian mixture model can be seen below: Example of clustering process on the Basel area Hence the matrix of probabilities of every point belonging to each cluster (\u03bb - what can be considered a matrix of weights) is multiplied by the mean of each cluster ( 1 x nc matrix mc ), forming the A matrix: or in matrices: Finally, the predictions can then be made using the sum of each row in the A matrix. It is important to state that the same crops (study areas) were used for this test. Although Caslano was not used in this case, as it possesses too few houses with a construction date below the oldest map available. Using the metric above explained a hold out cross-validation was performed, this way a group of points was only used for validation and not for training. After that, the RMSE (Root Mean Squared Error) was calculated using the difference between the date in the RBD database and the predicted one. This RMSE was also extrapolated to the whole Switzerland, so one could have a notion of what the overall error could be, using the following equation (for the expected error): where E is the error and n the number of buildings in each region. In addition to the RMSE , the 95th percentile was computed for every study area and using all combined as well. Hence, one could discuss the spread and predictability of errors.","title":"Methodology"},{"location":"TASK-REGBL/#results_1","text":"The first case analysed was Basel , where the final RMSE was 9.78 years. The density plot below demonstrates the distribution of errors in Basel , considering the difference between the year of construction in the RBD database and the predicted one. Distribution of error on construction year extrapolation Among the evaluated cases, Basel presented a strong visible spatial dependence, and it was also the case which the largest estimated proportion of houses with construction years older than (1955) the oldest map ( 11336 or approximately 66 % of buildings). Based on the validation dataset only, there was an overall trend of underestimation and the 95th percentile reached was 20 years, showing a not so spread and flat distribution of errors. Bern was the second case evaluated, and it demonstrated to be an atypical case. This starts from the fact that a big portion of the dates seemed incongruent with reality, based on the aerial images observed and as seen in the previous detection approach. Not only that, but almost 80 % of the buildings in Bern had missing data to what refers to the year of construction. This is especially complicated as the statistical method here presented is in essence an interpolator (intYEARpolator). Basically, as in any inference problem, data that is known is used to fill unknown data, therefore a reasonable split among known and unknown inputs is expected, as well as a considerable confidence on data. In the other hand, an estimated number of 1079 (approximately 27 % of the buildings) buildings was probably older than the oldest map available (1954) in Bern crop. Therefore, in one way liability was lower in this case, but the number of prediction points was smaller too. The following figure displays the density of errors in Bern, where an RMSE of 20.64 years was computed. Distribution of error on construction year extrapolation There was an overall trend for overestimation, though there was still enough lack of spread in errors, especially if one considers the 95th percentile of 42 . Finally, the crop on Biasca was evaluated. The computed RMSE was of 13.13 years, which is closer to the Basel case and the 95th percentile was 17 years, this way presenting the least spread error distribution. In Biasca an estimated 1007 ( 32 %) buildings were found, which is not much more than the proportion in Bern, but Biasca older topographic map used was from 1970, making of it an especially interesting case. The density plot below demonstrates the concentrated error case of Biasca : Distribution of error on construction year extrapolation Once the RMSE was computed for the three regions, it was extrapolated to the whole Switzerland by making consideration the size of each dataset: Extrapolation of the error distribution on the whole Switzerland The expected extrapolated error calculated was 15.6 years and the 95th percentile was then 31 years.","title":"Results"},{"location":"TASK-REGBL/#conclusion_1","text":"This add-on allows extrapolating the predictions to beyond the range of the topographical maps. Its predictions are limited, but the accuracy reached can be considered reasonable, once there is a considerable lack of information in this prediction range. Nor the dates in the RBD , nor the topographic maps can be fully trusted, ergo 15.6 years of error for the older buildings is acceptable, especially by considering the relative lack of spread in errors distribution. If a suggestion for improvement were to be given, a method for smoothing the intYEARpolator predictions could be interesting. This would possibly shift the distribution of the error into closer to a gaussian with mean zero. The dangerous found when searching for such an approach is that the year of construction of buildings does not seem to present a smooth surface, despite the spatial dependence. Hence, if this were to be considered, a balance between smoothing and variability would need to found. We also demonstrated a completely different perspective on how the spatial and temporal dimensions can be joined as the random variable predicted through spatial methodology was actually time. Therefore a strong demonstration of the importance of time in spatially related models and approaches was also given. The code for the intYEARpolator was developed in Python and it runs smoothly even with this quite big proportion of data. The singular case it can be quite time-demanding is in the case of high proportion of prediction points (missing values). It should also be reproducible to the whole Switzerland with no need for modification. A conditional argument is the use of concentric zones, that can be excluded in case of a total different pattern of processing time.","title":"Conclusion"},{"location":"TASK-REGBL/#reproduction-resources","text":"The source code of the proof-of-concept for national maps can be found here : National maps approach proof-of-concept (regbl-poc), STDL The README provides all the information needed to compile and use the proof-of-concept. The presented results and plots can be computed using the following tools suite : National maps approach results and plots (regbl-poc-analysis), STDL with again the README giving the instructions. The proof-of-concept source code for the statistical approach can be found here : Statistical approach proof-of-concept (regbl-poc-intyearpolator), STDL with its README giving the procedure to follow. The data needed to reproduce the national maps approach are not publicly available. For the national maps, a temporal series of the 1:25'000 maps of the same location are needed. They can be asked to swisstopp : GeoTIFF National Maps 1:25'000 rasters temporal sequence, swisstopo With the maps, you can follow the instruction for cutting and preparing them on the proof-of-concept README . The RBD data, used for both approaches, are not publicly available either. You can query them using the request form on the website of the Federal Statistical Office : DSV RBD data request form , Federal Statistical Office Both proof-of-concepts READMEs provide the required information to use these data.","title":"Reproduction Resources"},{"location":"TASK-REGBL/#references","text":"[1] Federal Statistical Office [2] Federal Register of Buildings and Dwellings [3] Federal Office of Topography [4] National Maps (1:25'000) [5] Conway, J. (1970), The game of life. Scientific American, vol. 223, no 4, p. 4. [6] Park, R. E.; Burgess, E. W. (1925). \"The Growth of the City: An Introduction to a Research Project\". The City (PDF). University of Chicago Press. pp. 47\u201362. ISBN 9780226148199. [7] Hoyt, H. (1939), The structure and growth of residential neighborhoods in American cities (Washington, DC). [8] Harris, C. D., and Ullman, E. L. (1945), \u2018The Nature of Cities\u2019, Annals of the American Academy of Political and Social Science, 242/Nov.: 7\u201317. [9] Duncan, B., Sabagh, G., & Van Arsdol,, M. D. (1962). Patterns of City Growth. American Journal of Sociology, 67(4), 418\u2013429. doi:10.1086/223165 [10] R\u00e9rat, P., S\u00f6derstr\u00f6m, O., Piguet, E., & Besson, R. (2010). From urban wastelands to new\u2010build gentrification: The case of Swiss cities. Population, Space and Place, 16(5), 429-442. [11] Kanevski, M., & Maignan, M. (2004). Analysis and modelling of spatial environmental data (Vol. 6501). EPFL press. [12] Diggle, P. J. Ribeiro Jr., P. J. (2007). Model-based Geostatistics. Springer Series in Statistics. [13] Montero, J. M., & Mateu, J. (2015). Spatial and spatio-temporal geostatistical modeling and kriging (Vol. 998). John Wiley & Sons. [14] Tobler, W. R. (1970). A computer movie simulating urban growth in the Detroit region. Economic geography, 46(sup1), 234-240.","title":"References"}]}